

\chapter{Directed networks}
In this chapter we will analize the traffic flow generated by a directed navigation on a point process defined on a $d$-dimensional hypercube. A directed navigation assigns a successor to each node of a point process who lies near a specific direction, iterating that process is how the traffic flow is generated. 

We will proof two main results. The first one measures how is the average behavior of the traffic flow on an $\mathbb{R}^{d-1}$ micro window orthogonal to the direction of the flow. On the second one we construct a directed navigation that satisfies the hypothesis of our first result. Both results follows a similar construction to Section 1.1 of \cite{WIAS} where the case of a $\mathbb{R}^{d-1}$ micro window which collapses in to a point is studied. Now we consider the case when the window does not collapse. 

\section{Directed navigation}
Let $X^{(s)}$ a simple point process with intensity $\lambda^{(s)}$ on $sC$ where $C$ is the interior of the $d$-dimensional hypercube $[-2, 2]^d$, $\lambda^{(s)}(x)=\lambda(\frac{x}{s})$ with \\$\lambda: C\rightarrow [0,\infty)$ a continuous and bounded mapping with positive maximum and minimum $\lambda_M, \lambda_m$ respectively. We assume that each node $X_i\in X^{(s)}$ generates traffic at rate $\mu^{(s)}(x)=\mu(\frac{x}{s})$ with $\mu: C\rightarrow [0,\infty)$ also a continuous and bounded mapping with maximum and minimum $\mu_M, \mu_m$ respectively.

We want to analyze the traffic flow who moves on a certain direction, for simplicity we will investigate a navigation on the direction $e_1$. For that let be $\pi_1:\mathbb{R}^d\rightarrow\mathbb{R}$ the projection to the first coordinate and $\textbf{N}$ the set of locally finite sets of points in $\mathbb{R}^d$.
\begin{defn} A measurable function $\mathcal{A}:\mathbb{R}^d\times\textbf{N}\rightarrow\mathbb{R}^d$ is called \textit{directed navigation} (on the direction $e_1$)  if for all $\varphi\in\textbf{N}$, $x\in\varphi$,
\begin{itemize}
\item $\mathcal{A}(x,\varphi)\in\varphi$, 
\item $\varphi(\mathcal{A}(x,\varphi))\geq \pi_1(x)$.
\end{itemize}
On the case of a point process $X$ for $X_{i}\in X$, we have that $\mathcal{A}(X_i, X)\in X$, so we will simply write $\mathcal{A}(X_i)$.
\end{defn}
\begin{figure}
\captionsetup{width=1\textwidth}
\centering
\includegraphics[width=1\textwidth]{directnavigation.jpg}
\caption{Directed navigation based on a Poisson pointed process on the unit square. Each node connects to its nearest neighbor which is also contained in a horizontal cone starting at the node. Figure took from \cite{WIAS}.}
\label{fig.2.1}
\end{figure}
In Figure \ref{fig.2.1} we present two examples of directed navigations, where the successor of a node is  the nearest neighbor on a cone with fixed angle pointing on the $e_1$ direction and whose vertex is $X_i$. Each picture represents a different angle.

Now we want to define the traffic flow at a node $X_i\in X^{(s)}$ with respect to $\mathcal{A}$, for that let $\mathcal{A}^k$ be the k-fold iteration of $\mathcal{A}$ with $\mathcal{A}^0(X_i)=X_i$ and $\mathcal{A}^k(X_i)=\mathcal{A}(\mathcal{A}^{k-1}(X_i))$, $k\geq 1$. So the \textit{trajectory} of a node $X_i\in X^{(s)}$ is $$\Gamma(X_i)=\lbrace \mathcal{A}^k(X_i):k\geq 0\rbrace. $$ In other words $\Gamma(X_i)$ consists all the iterated successors of $X_i$. 
Also the \textit{interpolated trajectory} is
$$\bar{\Gamma}(X_i)=\bigcup_{k\geq 0}[\mathcal{A}^{k}(X_i), \mathcal{A}^{k+1}(X_i)] $$ where $[x,y]$ is the line segment between $x$ and $y$. Finally we define the \textit{traffic flow} as $$\Delta(X_i)=\sum_{X_j: X_i\in\Gamma(X_j)}\mu^{(s)}(X_j). $$ Then the traffic flow over a node is generated by the nodes that after some iterations will arrive to it.

Now we want to specify the notion of a density that measures the number of links induced by the navigation $\mathcal{A}$ on a neighborhood of $sx$. For that let
$$I_s^D(x)=B_{g(s)}(sx)\cap\lbrace y\in\mathbb{R}^d:\pi(y)=\pi_1(sx)\rbrace $$ where $g(s)$ is a positive real valued function, denote the environment of $x$ inside the hyperplane through $x$ perpendicular to $e_1$. Let
$$\Xi_s^D(x)=\lbrace X_i\in X^{(s)}:[X_i,\mathcal{A}(X_i)]\cap I_s^D(x)\neq \emptyset\rbrace $$ be the set of points such that the segment $[X_i,\mathcal{A}(X_i)]$ crosses $I_s^D(x)$.
 
We are ready to define the \textit{link density condition}
 \begin{con}\label{con1} There exists a function $\lambda_\mathcal{A}(x):C\rightarrow(0,\infty)$ such that for every $x\in C$,
 $$\lambda_\mathcal{A}(x)=\lim_{s\rightarrow\infty}\frac{E\#\Xi_s^D(x) }{s\eta_{d-1}(I_s^D(x))}$$ where $\eta_{d-1}$ denotes the $(d-1)$-dimensional Hausdorff measure in $\mathbb{R}^d$.
 \end{con}
 
We write
 $$C_\varepsilon=\lbrace x\in C: B_\varepsilon(x)\subset C\rbrace $$ for the points of distance at least $\varepsilon$ to the boundary of $C$. Moreover, we denote by $$Z_r^D(x)=\mathbb{R}\times B_r^{d-1}(o)+x$$ the horizontal cylinder with radius $r$ shifted to the point $x\in\mathbb{R}^d$. We define $$E^D_{s,\varepsilon}=E^D_{s,\varepsilon,1}\cap E^D_{s,\varepsilon,2}$$ where $$E^D_{s,\varepsilon,1}=\lbrace\mathcal{A}(X_i)\neq X_i\mbox{ for all }X_i \in X^{(s)}\cap(sC)_{\varepsilon s}\rbrace$$ is the event that away from the boundary of $sC$, there are no \textit{dead ends}, that is no points $X_i\in X^{(s)}$ satisfying $\mathcal{A}(X_i)=X_i$ and $$E^D_{s,\varepsilon,2}=\lbrace(\bar{\Gamma}(X_i))\subset Z_{h(s)}^D(X_i)\mbox{ for all }X_i \in X^{(s)}\cap(sC)_{\varepsilon s}\rbrace $$ for $h(s)$ a positive real valued function, is the event that, away from the boundary of $sC$, all interpolated trajectories remain in the $h(s)$-cylinder at their starting points.  Let us now define the \textit{sub-ballisticity condition}.
 
 % On the figure we can observe a realization of a pointed process under the event $E^D_{s,\varepsilon}$
 
\begin{con}\label{con2} For all $\varepsilon >0$, we have $P((E_{s,\varepsilon}^D)^c)\in O(s^{-2d})$.
\end{con} 
 
%\begin{figure}
%\centering
%\includegraphics[width=.7\textwidth]{realization.jpg}
%\caption{A realization of the event $E^D_{s,\varepsilon}$. The traffic flow before $x$ inside $(sD)_{\varepsilon s}$ stays on the cylinder $Z_{h(s)}^D(x)$ and it finishes outside $(sD)_{\varepsilon s}$ at $y$.}
%\label{fig.2.2} 
%\end{figure}
\section{Convergence of the traffic flow average in the directed case} 
Now we are ready to enunciate and prove the main result of this chapter which states that under the link density and sub-ballisticity conditions we will have an almost surely convergence of the traffic flow average on a orthogonal micro environment to the flow.
The case when $g(s)/s$ tends monotonically to zero is studied in $\cite{WIAS}$ Theorem 2, now we will assume that $g(s)/s$ is constant getting a slightly different result.
  
\begin{thm}\label{thm2.1} Let $x\in C$, assume that Conditions \ref{con1}, \ref{con2} are satisfied for $g(s)=s$, $h(s)=s^\xi$,  $0<\xi<1$ and that $E[(\#X^{(s)})^2]\in O(s^{2d})$. 

1.Then, \small $$\lim_{s\rightarrow \infty}\frac{E\sum_{X_i\in\Xi^D_s(x)}\Delta(X_i)}{E\#\Xi^D_s(x)}=\frac{1}{\kappa_{d-1}\lambda_\mathcal{A}(x)}\int_{-\infty}^0\int_{B^{d-1}_{1}(o)}\mu(x+re_1+\hat{y})\lambda(x+re_1+\hat{y})d\bar{y}dr, $$\normalsize where $\hat{y}=(0, y_2, ...,y_d)$, $\bar{y}=(y_2, ...,y_d)$ and $\kappa_{d-1}$ denotes the volume of the unit ball in $\mathbb{R}^{d-1}$.

2. If additionally $X^{(s)}$ is either a Poisson point process or $\mu$ is constant and $X^{(s)}=X\cap sC$ for some ergodic point process $X$, then \small $$\lim_{s\rightarrow \infty}\frac{\sum_{X_i\in\Xi^D_s(x)}\Delta(X_i)}{E\#\Xi^D_s(x)}=\frac{1}{\kappa_{d-1}\lambda_\mathcal{A}(x)}\int_{-\infty}^0\int_{B^{d-1}_{1}(o)}\mu(x+re_1+\hat{y})\lambda(x+re_1+\hat{y})d\bar{y}dr$$\normalsize in probability.

\end{thm} 
 
\begin{proof}
Let us start with two lemmas giving estimates of the accumulated traffic flow under the events $E^D_{s,\varepsilon}$ and $(E^D_{s,\varepsilon})^c$. For this we need the following definitions. We write $R^+_s(x)=Z^D_{g(s)+h(s)}(sx)$ and $$R^{l,+}_s(x)=\lbrace sy\in sC\cap R^+_s(x): \pi_1(y)\leq\pi_1(x)\rbrace $$ for the set of points in $sC$ which lie in the microscopic cylinder $R^+_s(x)$ to the left of $sx$. Further we write $R^-_s(x)=Z^D_{g(s)-h(s)}(sx)$ and $$R^{l,-}_{s,\varepsilon}(x)=\lbrace sy\in sC\cap R^-_s(x): \zeta^-(\varepsilon)\leq\pi_1(y)\leq\pi_1(x)\rbrace $$ where $$\zeta^-(\varepsilon)=\inf\lbrace\pi(z):z\in\partial C_{\varepsilon}\cap (R^-_s(x)/s)\rbrace$$ denotes the infimum of the projection of the intersection of $\partial C_{\varepsilon}$ with the microscopic cylinder $R^-_s(x)/s$ that is orthogonal to the horizontal axis, it is important to remark that since $C$ is an hypercube this value does not depend  on $s$, see also Figure \ref{fig.2.3}. 
\begin{figure}
\captionsetup{width=1\textwidth}
\centering
\includegraphics[width=.7\textwidth]{cylinder.jpg}
\caption{Construction of the cylinders $R^{l,-}_{s,\varepsilon}(x)$ (light gray) and $R^{l,+}_s(x)$ (union of light and dark gray). After re-scaling $\zeta^-(\varepsilon)$ does not depend on $s$.}
\label{fig.2.3}
\end{figure}
 \begin{lem}\label{lem2.1}
Let $\varepsilon>0$ and $x\in C_{2\varepsilon}$. Furthermore let $X^{(s)}$ on the event $E_{s,\varepsilon}^D$ then
 $$\sum_{X_j\in X^{(s)}\cap R^{l,-}_{s,\varepsilon}(x)}\mu^{(s)}(X_j)\leq\sum_{X_j\in  \Xi^{D}_{s}(x)}\Delta^{(s)}(X_j)\leq\sum_{X_j\in X^{(s)}\cap R^{l,+}_{s}(x)}\mu^{(s)}(X_j).$$
 \end{lem}
 \begin{proof}
For the upper bound, the cylinder condition given by $E^D_{s,\varepsilon}$ ensures that we can estimate
\begin{align}
&\sum_{X_i\in X^{(s)}\cap \Xi^D_s(x)}\Delta (x)(X_i)=\sum_{X_j\in X^{(s)}}\mu^{(s)}(X_j)\sum_{X_i\in \Xi^D_s(x)}\mathds{1}_{\Gamma(X_j)}(X_i)\nonumber\\
&=\sum_{X_i\in X^{(s)}\cap R_{s}^{l,+}(x)}\mu^{(s)}(X_j)\sum_{X_i\in \Xi^D_s(x)}\mathds{1}_{\Gamma(X_j)}(X_i)\leq \sum_{X_i\in X^{(s)}\cap R_{s}^{l,+}(x)}\mu^{(s)}(X_j).\nonumber
\end{align}
For the lower bound, since $E^D_{s,\varepsilon}$ does not give control over nodes in $sC\setminus (sC)_{s\varepsilon}$, those nodes have to be excluded.

 \end{proof}
 \begin{lem}\label{lem2.2} $$E(\mathds{1}_{(E^{D}_{s,\varepsilon})^c}\sum_{X_i\in\Xi_s^D(x)}\Delta(X_i))\leq E(\mathds{1}_{(E^{D}_{s,\varepsilon})^c}\sum_{X_i\in X^{(s)}}\mu(X_i))\in O(1).$$
 \end{lem}
 \begin{proof}
 Since $\mathcal{A}$ is a directed navigation we have that for distinct $X_{i}, X_{k}\in  \Xi^{D}_{s}(x)$, the intersection of the sets $$\lbrace X_j: X_{i} \in \Gamma(X_j)\rbrace, \lbrace X_j: X_{k} \in \Gamma(X_j)\rbrace$$ is empty. This implies that under $(E^{D}_{s,\varepsilon})^c$, $$\sum_{X_i\in \Xi^{D}_{s}(x)}\Delta^{(s)}(X_i)\leq\sum_{X_i\in X^{(s)}}\mu^{(s)}(X_i)\leq \mu_{M}\#X^{(s)} $$ and hence by Cauchy-Schwartz inequality, since $E[(\#X^{(s)})^2]\in O(s^{2d})$ and Condition \ref{con2},
\begin{align*}
E(\mathds{1}_{(E^{D}_{s,\varepsilon})^c}\sum_{X_i\in\Xi_s^D(x)}\Delta^{(s)}(X_i))\leq E(\mathds{1}_{(E^{D}_{s,\varepsilon})^c}\sum_{X_i\in X^{(s)}}\mu^{(s)}(X_i))\\
\leq \mu_M \sqrt{(P(E^D_{s,\varepsilon}))^c}\sqrt{E[(\#X^{(s)})^2]}\in O(1).
\end{align*}

 \end{proof}

We are ready to proof the theorem. Let us abbreviate $i_s=s\eta_{d-1}(I^D_s(x))$, $$N_s=i_s^{-1}\sum_{X_i\in\Xi_s^D(x)}\Delta(X_i), S=\frac{1}{\kappa_{d-1}}\int_{-\infty}^0\int_{B^{d-1}_{1}(o)}\mu(x+re_1+\hat{y})\lambda(x+re_1+\hat{y})d\bar{y}dr.$$
 
 \textit{Proof of Theorem.}\textbf{1.} By Condition \ref{con1} it is enough to proof that $$\lim_{s\rightarrow\infty}EN_s=S.$$ We have that
 $$EN_s=E\mathds{1}_{E^{D}_{s,\varepsilon}}N_s+E\mathds{1}_{(E^{D}_{s,\varepsilon})^c}N_s .$$
 On the one hand since $$i_s=s\eta_{d-1}((I^D_s(x))=sg(s)^{d-1}\kappa_{d-1}=s^{d}\kappa_{d-1},$$ then $i_s^{-1}\in o(1) $. So by Lemma \ref{lem2.2} $E\mathds{1}_{(E^{D}_{s,\varepsilon})^c}N_s\in o(1)$.

On the other hand by Lemma \ref{lem2.1}, Campbell's theorem and the coordinate transformation $y=sz$,

 \begin{align}
 E\mathds{1}_{E^{D}_{s,\varepsilon}}N_s&\leq i_s^{-1}E\sum_{X_j\in X^{(s)}\cap R^{l,+}_{s}(x)}\mu^{(s)}(X_j)\nonumber\\
 &=i_s^{-1}\int_{R^{l,+}_{s}(x)}\mu^{(s)}(y)\lambda^{(s)}(y)dy\nonumber\\
 &=s^{d}i_s^{-1}\int_{R^{+}(x)/s}\mathds{1}_{\pi_1(y)\leq\pi_1(x)}\mu(y)\lambda(y)dy\nonumber,
 \end{align}
Hence, by Fubini's theorem, we have for $\hat{y}=(0, y_2, ...,y_d)$, $\bar{y}=(y_2, ...,y_d)$

\begin{align}
E\mathds{1}_{E^{D}_{s,\varepsilon}}N_s&\leq\frac{s^{d-1}}{g(s)^{d-1}\kappa_{d-1}}\int_{-\infty}^0\int_{B^{d-1}_{\frac{(h(s)+g(s))}{s}}(o)}\mu(x+re_1+\hat{y})\lambda(x+re_1+\hat{y})d\bar{y}dr\nonumber\\
&=\frac{1}{\kappa_{d-1}}\int_{-\infty}^0\int_{B^{d-1}_{1+s^{\xi-1}}(o)}\mu(x+re_1+\hat{y})\lambda(x+re_1+\hat{y})d\bar{y}dr.\nonumber
\end{align} 
And by the Dominated Convergence theorem we get the upper bound.

For the lower bound lets pick $s$ enough large that $g(s)-h(s)>0$. By Lemma \ref{lem2.1} we can estimate
\begin{align}
 i_s^{-1}E\sum_{X_j\in X^{(s)}\cap R^{l,-}_{s,\varepsilon}(x)}\mu^{(s)}(X_j) -i_s^{-1}E\mathds{1}_{(E^{D}_{s,\varepsilon})^c}\sum_{X_j\in X^{(s)}}\mu^{(s)}(X_j)\leq E\mathds{1}_{E^{D}_{s,\varepsilon}}N_s\nonumber.
 \end{align} 
The second summand tends to zero when $s$ tends to infinity since $i_s^{-1}\in o(1)$ and by Lemma \ref{lem2.2}. 

Lets proceed to estimate the first one. Again by Campbell's theorem and the coordinate transformation $y=sz$,

 \begin{align}
 i_s^{-1}&E\sum_{X_j\in X^{(s)}\cap R^{l,+}_{s,\varepsilon}(x)}\mu^{(s)}(X_j)
=i_s^{-1}\int_{R^{l,-}_{s,\varepsilon}(x)}\mu^{(s)}(y)\lambda^{(s)}(y)dy\nonumber\\
 &=s^{d}i_s^{-1}\int_{R^{-}(x)/s}\mathds{1}_{\pi_1(y)\leq\pi_1(x)}\mu(y)-s^{d}i_s^{-1}\int_{R^{-}(x)/s}\mathds{1}_{\pi_1(y)<\zeta_s^-(\varepsilon)}\mu(y)\lambda(y)dy\nonumber,
 \end{align}
By an analogous reasoning to the upper bound the first summand converges to $S$. 

For the second summand lets observe that
\small
 $$s^d i_s^{-1}\int_{R^{-}(x)/s}\mathds{1}_{\pi_1(y)<\zeta_s^-(\varepsilon)}\mu(y)\lambda(y)dy\leq \mu_M \lambda_M\frac{\eta_{d-1}(B^{d-1}_{g(s)-h(s)}(o))}{\kappa_{d-1}}(\zeta^-(\varepsilon)-\zeta^-) $$
\normalsize
where $\zeta^-=\inf \lbrace\pi_1(z), z\in \partial C\cap(R^-_s(x)/s)\rbrace$ denotes the smallest first coordinate of points on the boundary of $S$ intersected with the cylinder $R_s^-(x)/s$ and also does not depend on $s$ since $C$ is an hypercube. To conclude that the upper bound of the inequality tends to zero, note that 
$$\lim_{s\rightarrow\infty}\frac{\eta_{d-1}(B^{d-1}_{g(s)-h(s)}(o))}{\kappa_{d-1}}=\lim_{s\rightarrow\infty}(g(s)-h(s))^{d-1}=1. $$ And finally when $\varepsilon$ tends to zero, $\zeta^-(\varepsilon)$ tends to $\zeta^-$. 

%let $x^-$, $x^-_\varepsilon$ denote the intersections of the negative horizontal ray $x-(0,\infty)e_1$ with $\partial D$ and $\partial D_\varepsilon$ respectively, this points are unique due to the convexity of D (saw Figure \ref{fig.2.3}). Now when $s$ tends to infinity since $g(s)-h(s)$, $\zeta_s^-$ tends to $\pi_1(x^-)$ and $\zeta_s^-(\varepsilon)$ tends to $\pi_1(x^-_\varepsilon)$. The result follows from the fact that $$\lim_{\varepsilon\rightarrow 0}x-_\varepsilon=x^-. $$

\textbf{2.} Again by Condition \ref{con1} it is enough to proof that $$\lim_{s\rightarrow\infty}N_s=S\mbox{ in probability}.$$  By Markov's inequality $$P(\vert N_s-S\vert >\varepsilon)\leq \varepsilon^{-1}E(\vert N_s-S\vert), $$ so we will prove that the upper bound tends to zero when $s$ tends to infinity. We start by estimating
$$E(\vert N_s-S\vert)\leq E(\vert N_s-EN_s\vert)+E(\vert EN_s-S\vert) $$
where by the Dominated Convergence theorem and 1 the second summand tends to zero. Furthermore we can write $$E(\vert N_s-EN_s\vert)=E(\mathds{1}_{E^D_{s,\varepsilon}}\vert N_s-EN_s\vert)+E(\mathds{1}_{(E^D_{s,\varepsilon})^c}\vert N_s-EN_s\vert). $$ 

On the one hand the second summand can be bounded  $$E(\mathds{1}_{(E^D_{s,\varepsilon})^c}\vert N_s-EN_s\vert)\leq E(\mathds{1}_{(E^D_{s,\varepsilon})^c} N_s)+E\mathds{1}_{(E^D_{s,\varepsilon})^c}EN_s. $$ By Lemma \ref{lem2.2} the first term is in $o(1)$, by Condition \ref{con2} and 1. $$\lim_{s\rightarrow\infty}E\mathds{1}_{(E^D_{s,\varepsilon})^c} EN_s =0\cdot S=0.$$ 
 
On the other hand by Lemma \ref{lem2.1} the first summand can be bounded
\begin{align}
E(\mathds{1}_{E^D_{s,\varepsilon}\cap\lbrace N_s\geq EN_s\rbrace} (N_s-EN_s))&+E(\mathds{1}_{E^D_{s,\varepsilon}\cap\lbrace  EN_s\geq N_s \rbrace}(EN_s-N_s))\nonumber\\
&\leq E(\vert N_s^+-EN_s\vert)+E(\vert N_s^{-,\varepsilon}-EN_s\vert),\nonumber
\end{align}
where we write 
$$N^+_s=i^{-1}_s\sum_{X_j\in X^{(s)}\cap R^{l,+}_{s}(x)}\mu^{(s)}(X_j)\mbox{ and }N^{-,\varepsilon}_s=i^{-1}_s\sum_{X_j\in X^{(s)}\cap R^{l,-}_{s,\varepsilon}(x)}\mu^{(s)}(X_j).$$ Finally we have that 
\begin{align}
E(\vert N_s^+-EN_s\vert)&\leq E(\vert N_s^+-EN_s^+\vert)+E(\vert EN_s^+-E N_s\vert)\nonumber\\
E(\vert N_s^{-,\varepsilon}-EN_s\vert)&\leq E(\vert N_s^{-,\varepsilon}-EN_s^{-,\varepsilon}\vert)+E(\vert EN_s^{-,\varepsilon}-E N_s\vert).\nonumber
\end{align}
On 1 we showed that $$\lim_{s\rightarrow\infty}\vert EN_s^+-E N_s\vert=\lim_{\varepsilon\rightarrow 0}\lim_{s\rightarrow\infty}\vert EN_s^{-,\varepsilon}-EN_s\vert=0.$$ So we have to find estimates for $E(\vert N_s^+-EN_s^+\vert)$ and $E(\vert N_s^{-,\varepsilon}-EN_s^{-,\varepsilon}\vert)$. This will be done distinguishing between cases.

\textbf{Case 1)} Let us start with the case of a Poisson point process $X^{(s)}$. Then by Jensen's inequality,  Campbell's theorem for the variance and Fubini's theorem we have
\begin{align}
E(\vert N_s^+&-EN_s^+\vert)\leq(VN^+_s)^{1/2}=i^{-1}_s V(\sum_{X_j\in X^{(s)}\cap R^{l,+}_{s}(x)}\mu^{(s)}(X_j))^{1/2}\nonumber\\
&=i^{-1}_s(s^d\int_{R^+_s(x)/s}\mathds{1}_{\pi_1(y)\leq \pi_1(x)}\mu(y)^2\lambda(y)dy)^{1/2}\nonumber\\
&=((i^{-1}_s)^2s^d\int_{-\infty}^0\int_{B^{d-1}_{\frac{(h(s)+g(s))}{s}}(o)}\mu(x+re_1+\hat{y})^2\lambda(x+re_1+\hat{y})d\bar{y}dr)^{1/2}\nonumber.
\end{align}
For finishing lets observe that
\begin{align}
(i^{-1}_s)^2& s^d\int_{B^{d-1}_{\frac{(h(s)+g(s))}{s}}(o)}\mu(x+re_1+\hat{y})^2\lambda(x+re_1+\hat{y})d\bar{y}\nonumber\\ &\leq \frac{s^d\eta_{d-1}(B^{d-1}_{\frac{(h(s)+g(s))}{s}}(o))}{(s^d\kappa_{d-1})^2}\mu_M^2\lambda_M
=\frac{\mu_M^2\lambda_M}{\kappa_{d-1}s^d}(\frac{s+s^\xi}{s})^{d-1}\nonumber,
\end{align}
which tends to zero when $s$ tends to infinity. The argument for $N^{-,\varepsilon}$ is analogous.

\textbf{Case 2)} Since $X$ is ergodic $\lambda$ is constant, by hypothesis $\mu$ is also constant, let us abbreviate $r^+_s(x)=\eta_d(R^{l,+}_s(x))$. Then by Campbell's theorem
\begin{align}
E(\vert N_s^+-EN_s^+\vert)&=i^{-1}_s E(\vert\#(X\cap R_{s}^{l,+}(x))\mu-r^+_s(x)\lambda\mu\vert)\nonumber\\
&=\mu r^+_s(x)i_s^{-1} E(\vert\frac{\#(X\cap R_{s}^{l,+}(x))}{r^+_s(x)}-\lambda\vert),\nonumber
\end{align}
where by analogous computations to 1. $$\lim_{s\rightarrow\infty} r^+_s(x)i_s^{-1}=(\pi_1(x)-\zeta^-).$$ 

Since $X$ is translation invariant, $R_s^{l,+}$ can be replaced by
$$R_{s,x}^{l,+}=\lbrace sy\in \partial(C-x)\cap R_s^+(o):\pi_1(y)\leq 0\rbrace, $$
which since $C$ is an hypercube is a is a sequence of convex averaging windows. Hence 
$$E(\vert\frac{\#(X\cap R_{s}^{l,+})}{r^+_s(x)}-\lambda\vert)=E(\vert\frac{\#(X\cap R_{s, x}^{l,+})}{r^+_s(x)}-\lambda\vert, $$
which by the ergodic theorem tends to zero when s tends to infinity. The argument for $N^{-,\varepsilon}$ is analogous. 
 \end{proof}
 \section{Directed spanning tree navigation}
 We will give an example of a directed navigation that satisfies the Conditions \ref{con1} and \ref{con2} on a Poisson point process.
 \begin{defn}[Spanning tree navigation] Let $\mathcal{A}:\mathbb{R}^{d}\times\textbf{N}\rightarrow \textbf{N}$, $\varphi\in\textbf{N}$ and $x\in\mathbb{R}^{d}$. $\mathcal{A}(x,\varphi)$ is defined to be the point in $\varphi\cap((\pi_1(x),\infty)\times\mathbb{R}^{d-1})$ that is of minimal Euclidean distance to $x$. If the minimum is realized in several points, we choose the lexicographic smallest of them. If there is no point of $\varphi$ to the right of $x$, then we put $\mathcal{A}(x,\varphi)=x$.
 \end{defn} 
 
The spanning tree is an example of a directed navigation, from now until the end of this chapter we will use $\mathcal{A}$ to refer it. Firs we will proof that $\mathcal{A}$ satisfies Condition \ref{con1} in \cite{WIAS} Proposition 5 is considered the case where $g(s)=s^\xi$, $0<\xi<1$. Here we follow similar techniques to prove the next result.

\begin{prop} Let $\lambda: C\rightarrow [0,\infty)$ a locally Lipschitz  and bounded mapping. Further, let $X^{(s)}$ be a Poisson point process on $sC$ with intensity function $\lambda^{(s)}$. Then Condition \ref{con1} is satisfied for the directed spanning tree on $X^{(s)}$ together with the fluctuation function $g(s)=s$.
\end{prop}
\begin{proof}
First we will show three auxiliary lemmas. We will simplify the notation writing $I_s(x)=I^D_s(x)$ and we will use $X^{[\lambda(x)]}$ for the marked process of constant intensity function $\lambda(x)$ which construction was explained in Proposition \ref{prop1.1}. Also for $\xi'\in(1,2)$ we write $I^+_s(x)=I_s(x)\oplus B^d_{s^{2-\xi'}}(o) $ where $\oplus$ denotes the Minkowski's sum. Our first result analyze the crossings out of $I^+_s(x)$ when $s$ tends to infinity.
\begin{lem}\label{lem2.3} Let $x\in C$ be arbitrary. Then,
\begin{enumerate}
\item $\int_{sC\setminus I^+_s(x)}P([y, \mathcal{A}(y,X^{(s)}\cup\lbrace y\rbrace)]\cap I_s(x)\neq \emptyset)\lambda^{(s)}(y)dy\in o(s\eta_{d-1}(I_s(x)))$,
\item $\int_{sC\setminus I^+_s(x)}P([y, \mathcal{A}(y,X^{[\lambda(x)]}\cup\lbrace y\rbrace)]\cap I_s(x)\neq \emptyset)dy\in o(s\eta_{d-1}(I_s(x)))$.

\end{enumerate}

\end{lem}
\begin{proof}
\textbf{1.} Let $\varepsilon>0$ be such that $B_{3\varepsilon}(x)\subset C$. If $y\in sC\setminus B_{2\varepsilon s}(sx)$ is such that $[y, \mathcal{A}(y,X^{(s)}\cup\lbrace y\rbrace)]\cap I_s(x)\neq \emptyset$ then $X^{(s)}$ does not contain any points in the set 
$$B_{\vert y-sx\vert -\varepsilon s}(y)\cap B_{2\varepsilon s}(sx)\cap ([\pi_1 (y), \infty)\times\mathbb{R}^{d-1})$$
which contains half ball of radius $2^{-1}\varepsilon s.$  Hence since $X^{(s)}$ is a Poisson point process of intensity $\lambda^{(s)}$ we can estimate,
$$P([y, \mathcal{A}(y,X^{(s)}\cup\lbrace y\rbrace)]\cap I_s(x)\neq \emptyset)\leq \exp(-\lambda_m\kappa_d 2^{-1}(2^{-1}\varepsilon s)^d), $$ where $\kappa_d$ denotes the volume of the unit ball in $\mathbb{R}^d$. And we can bound
\begin{align}
\int_{sC\setminus B_{2\varepsilon s}(sx)}&P([y, \mathcal{A}(y,X^{(s)}\cup\lbrace y\rbrace)]\cap I_s(x)\neq \emptyset)\lambda^{(s)}(y)dy\nonumber\\
&\leq\exp(-\lambda_m\kappa_d 2^{-1}(2^{-1}\varepsilon s)^d)\eta_{d}(sC)\nonumber\\
&=\eta_{d}(C)\exp(-\lambda_m\kappa_d 2^{-1}(2^{-1}\varepsilon s)^d)s^{d}\in o(1).\nonumber
\end{align}
Which implies that $$\int_{sC\setminus B_{2\varepsilon s}(sx)}P([y, \mathcal{A}(y,X^{(s)}\cup\lbrace y\rbrace)]\cap I_s(x)\neq \emptyset)\lambda^{(s)}(y)dy\in o(s\eta_{d-1}(I_s(x))),$$ since for $s$ big enough $s\eta_{d-1}(I_s(x))\geq 1$.

For finishing the claim, we have to proof that
 $$\int_{B_{2\varepsilon s}(sx)\setminus I^+_s(x)}P([y, \mathcal{A}(y,X^{(s)}\cup\lbrace y\rbrace)]\cap I_s(x)\neq \emptyset)\lambda^{(s)}(y)dy\in o(s\eta_{d-1}(I_s(x))).$$ Let $y\in B_{2\varepsilon s}(sx)\setminus I^+_s(x)$ such that $[y, \mathcal{A}(y,X^{(s)}\cup\lbrace y\rbrace)]\cap I_s(x)\neq \emptyset$ then $\vert y-\mathcal{A}(y,X^{(s)}\cup\lbrace y\rbrace)\vert\geq s^{2-\xi'} $ and we can estimate
\begin{align}
\int&_{B_{2\varepsilon s}(sx)\setminus I^+_s(x)}P([y, \mathcal{A}(y,X^{(s)}\cup\lbrace y\rbrace)]\cap I_s(x)\neq \emptyset)\lambda^{(s)}(y)dy\nonumber\\
&\leq \int_{B_{2\varepsilon s}(sx)}P(\vert y-\mathcal{A}(y,X^{(s)}\cup\lbrace y\rbrace)\vert\geq s^{2-\xi'})\lambda^{(s)}(y)dy\nonumber.
\end{align}
The event $\vert y-\mathcal{A}(y,X^{(s)}\cup\lbrace y\rbrace)\vert\geq s^{2-\xi'}$ implies that half a ball of radius $s^{2-\xi'}$ contains no points so we can bound
\begin{align}
\int_{B_{2\varepsilon s}(sx)}&P(\vert y-\mathcal{A}(y,X^{(s)}\cup\lbrace y\rbrace)\vert\geq s^{2-\xi'})\lambda^{(s)}(y)dy\nonumber\\
&\leq\exp(-\lambda_m\kappa_d2^{-1}(s^{2-\xi'})^d)\eta_{d}(B_{2\varepsilon s}(sx))\nonumber\\
&=\kappa_d\exp(-\lambda_m\kappa_d2^{-1}(s^{2-\xi'})^d)(2\varepsilon s)^{d}\in o(1).\nonumber
\end{align}
Which implies that 
$$\int_{B_{2\varepsilon s}(sx)}P(\vert y-\mathcal{A}(y,X^{(s)}\cup\lbrace y\rbrace)\vert\geq s^{2-\xi'})\lambda^{(s)}(y)dy\in o(s\eta_{d-1}(I_s(x))) $$ since for $s$ big enough $\eta_{d-1}(s I_s(x))\geq 1$.

\textbf{2.} It is implied from 1 since $X^{[\lambda(x)]}$ has constant intensity function $\lambda^{(s)}(y)=\lambda(x)$.
\end{proof}

We show that if we replace integrals over $I^+_s(x)$ with respect to the intensity $\lambda^{(s)}$ by integrals with respect to the constant intensity $\lambda(x)$, then the error is of order $o(s\eta_{d-1}(I_s(x)))$.

\begin{lem}\label{lem2.4} Let $x\in C$ be arbitrary and let $f:I^+_s(x)\rightarrow[0,1]$ be a measurable function. Then, $$\vert \int_{I^+_s(x)}f(y)\lambda^{(s)}(y)dy-\lambda(x)\int_{I^+_s(x)}f(y)dy\vert\in o(s\eta_{d-1}(I_s(x)) $$
\end{lem}
\begin{proof}
Let $L$ be the Lipschitz constant of $\lambda$ in $C$. Since the biggest distance from between two points in $I_s^+(x)$ is $2(g(s)+s^{2-\xi'})=2(s+s^{2-\xi'})$, for $y\in I_s^+(x)$ we can estimate
$$\vert \lambda^{(s)}(y)-\lambda(x)\vert=\vert \lambda(\frac{y}{s})-\lambda(x)\vert\leq 2L(s+s^{2-\xi'})s^{-1}. $$ Hence $$\int_{I^+_s(x)}f(y)\vert\lambda^{(s)}(y)-\lambda(x)\vert dy \leq 2L(s+s^{2-\xi'})s^{-1}\eta_d(I^+_s(x)). $$ Finally lets observe that since $o(\eta_{d}(I^+_s(x)))=o(s^{2-\xi'}\eta_{d-1}(I_s(x)))$, after dividing by $s\eta_{d-1}(I_s(x))$ we get the bound
$$2L(s^{1-\xi'}+s^{2(1-\xi')})$$ which tends to zero when $s$ tends to infinity.
\end{proof}

Finally we show that replacing the Poisson point process $X^{(s)}$ by the stationary Poisson point process $X^{[\lambda(x)]}$ in $$P([y, \mathcal{A}(y,X^{(s)}\cup\lbrace y\rbrace)]\cap I_s(x)\neq \emptyset)$$ leads to a negligible error.

\begin{lem}\label{lem2.5} Let $x\in C$ be arbitrary. Then 
\small $$\int_{I^+_s(x)}\vert P([y, \mathcal{A}(y,X^{(s)}\cup\lbrace y\rbrace)]\cap I_s(x)\neq \emptyset)-P([y, \mathcal{A}(y,X^{[\lambda(x)]}\cup\lbrace y\rbrace)]\cap I_s(x)\neq \emptyset)\vert dy$$
\normalsize
is of order $o(s\eta_{d-1}(I_s(x))$.
\end{lem}
\begin{proof}
Let $I_s^{++}(x)=I_s^+\oplus B_{s^{2-\xi'}}$ and $\alpha d<-1$. Let $y\in I_s^+(x)$ and lets assume that $[y, \mathcal{A}(y,X^{(s)}\cup\lbrace y\rbrace)]\cap I_s(x)\neq \emptyset$. Then we have two possibilities
\begin{enumerate}
\item $[y, \mathcal{A}(y,X^{(s)}\cup\lbrace y\rbrace)]\neq [y, \mathcal{A}(y,X^{[\lambda (x)]}\cup\lbrace y\rbrace)]$,
\item $[y, \mathcal{A}(y,X^{(s)}\cup\lbrace y\rbrace)]= [y, \mathcal{A}(y,X^{[\lambda (x)]}\cup\lbrace y\rbrace)]$ which implies that\\ $[y, \mathcal{A}(y,X^{[\lambda (x)]}\cup\lbrace y\rbrace)]\cap I_s(x)\neq \emptyset$.
\end{enumerate}
We can follow an analogous reasoning exchanging $X^{(s)}$ by $X^{[\lambda (x)]}$. As a consequence we estimate
\begin{align}
\vert &P([y, \mathcal{A}(y,X^{(s)}\cup\lbrace y\rbrace)]\cap I_s(x)\neq \emptyset)-P([y, \mathcal{A}(y,X^{\lambda(x)}\cup\lbrace y\rbrace)]\cap I_s(x)\neq \emptyset)\vert\nonumber\\
&\leq P([y, \mathcal{A}(y,X^{(s)}\cup\lbrace y\rbrace)]\neq [y, \mathcal{A}(y,X^{[\lambda (x)]}\cup\lbrace y\rbrace)])\nonumber\\
&\leq P(X^{[\lambda (x)]}\cap B_{s^\alpha}(y)\neq X^{(s)}\cap B_{s^\alpha}(y))+P(\vert y-\mathcal{A}(y, X^{[\lambda(x)]}\cup\lbrace y\rbrace)\vert\geq s^{\alpha}).\nonumber
\end{align}

On the one hand, lets estimate the first summand, hence lets remember that we can write 
$$X^{(s)}=\lbrace X_i:(X_i, U_i)\in X\mbox{ and }U_i\leq\lambda^{(s)}(X_i)\rbrace, $$
$$X^{[\lambda(x)]}=\lbrace X_i:(X_i, U_i)\in X\mbox{ and }U_i\leq\lambda (x)\rbrace,$$
which by elemental computations implies that $$X^{(s)}\triangle X^{[\lambda(x)]}\subset\lbrace X_i:(X_i, U_i)\in X\mbox{ and }U_i\leq\vert\lambda^{(s)}(X_i)-\lambda(x)\vert\rbrace$$ where $\triangle$ denotes the symmetrical difference.

The Lipschitz continuity implies that $$\sup_{y\in I_s^{++}(x)}\vert \lambda^{(s)}(y)-\lambda(x)\vert\leq 2L(s+2s^{2-\xi'})s^{-1}. $$ Hence by Campbell's theorem,

\begin{align}
\int_{I^+_s(x)}&P(X^{[\lambda (x)]}\cap B_{s^\alpha}(y)\neq X^{(s)}\cap B_{s^\alpha}(y))dy\nonumber\\
&\leq\int_{I^+_s(x)} E\#(X^{[\vert \lambda(\cdot)-\lambda(x)\vert]}\cap B_{s^\alpha}(y))dy\nonumber\\
&\leq\int_{I^+_s(x)}2L(s+2s^{2-\xi'})s^{-1} \eta_d(B_{s^\alpha}(y))dy\nonumber\\
&=2L\kappa_d  \eta_d(I^+_s(x))(s+2s^{2-\xi'})s^{\alpha d -1}.\nonumber
\end{align}

Since $o(\eta_{d}(I^+_s(x)))=o(s^{2-\xi'}\eta_{d-1}(I_s(x)))$, after dividing by $\eta_{d-1}(I_s(x))$ we get the bound
$$2L\kappa_d  (s^{\alpha d+2-\xi'}+2s^{\alpha d+3-2\xi'})$$ which due to the conditions imposed to $\alpha$ tends to zero as $s$ tends to infinity.

On the other hand, by since $X^{[\lambda (x)]}$ is stationary
\small $$\int_{I^+_s(x)}P(\vert y-\mathcal{A}(y, X^{[\lambda(x)]}\cup\lbrace y\rbrace)\vert\geq s^{\alpha}) dy=P(\vert\mathcal{A}(o, X^{[\lambda(x)]}\cup\lbrace o\rbrace)\vert\geq s^{\alpha})\eta_d(I^+_s(x)).$$
\normalsize
And as in the proof of Lemma \ref{lem2.3} we can estimate 
$$\int_{I^+_s(x)}P(\vert\mathcal{A}(o, X^{[\lambda(x)]}\cup\lbrace o\rbrace)\vert\geq s^{\alpha})dy\leq\exp(-\lambda_m\kappa_d2^{-1}s^{d\alpha})\eta_{d}(I^+_s(x)).$$ Hence after dividing by $o(s\eta_{d-1}(I_s(x))$ we get the bound $$\exp(-\lambda_m\kappa_d2^{-1}(s^{d\alpha}))s^{1-\xi'} $$ which tends to zero when $s$ tends to infinity.
\end{proof}
\textit{Proof of Proposition.}
Let $x\in C$, we claim that 
$$\lambda_{\mathcal{A}}(x)=\frac{\lambda (x)\int_{C}P([y, \mathcal{A}(y, X^{[\lambda(x)]}\cup\lbrace y\rbrace)]\cap I_1(x)\neq\emptyset)dy}{\eta_{d-1}(I_1(x))}. $$ 
On the one hand by Slyvniak-Mecke's theorem we get that
\begin{align}
E&\#\lbrace X_i\in X^{[\lambda (x)]}\cap sC: [X_i, \mathcal{A}(X_i, X^{(s)})]\cap I_s(x)\neq\emptyset\rbrace\nonumber\\
&=\lambda(x)\int_{sC}P([y, \mathcal{A}(y, X^{[\lambda (x)]}\cup\lbrace y\rbrace)]\cap I_s(x)\neq\emptyset)dy\nonumber.
\end{align}
Since $X^{[\lambda (x)]}$ is stationary we have that $$\frac{\lambda (x)\int_{sC}P([y, \mathcal{A}(y, X^{[\lambda(x)]}\cup\lbrace y\rbrace)]\cap I_s(x)\neq\emptyset)dy}{s\eta_{d-1}(I_s(x))}$$ is constant for all $s$.
On the other hand, also by Slyvniak-Mecke's theorem
\begin{align}
E\#\Xi_s^D(x) &=E\#\lbrace X_i\in X^{(s)}: [X_i, \mathcal{A}(X_i, X^{(s)})]\cap I_s(x)\neq\emptyset\rbrace\nonumber\\
&=\int_{sC}P([y, \mathcal{A}(y, X^{(s)}\cup\lbrace y\rbrace)]\cap I_s(x)\neq\emptyset)\lambda^{(s)}(y)dy\nonumber.
\end{align}
Therefore, by Lemma \ref{lem2.3}, it suffices to show that the difference
\begin{align}
\int_{I^+_s(x)} P([y, \mathcal{A}(y,X^{(s)}\cup\lbrace y\rbrace)]\cap I_s(x)\neq \emptyset)\lambda^{(s)}(y)dy\nonumber\\
-\int_{I^+_s(x)} P([y, \mathcal{A}(y,X^{[\lambda(x)]}\cup\lbrace y\rbrace)]\cap I_s(x)\neq \emptyset)\lambda(x)dy\nonumber
\end{align}
is of order $o(s\eta_{d-1}(I_s(x)))$. This follows by the triangle inequality and Lemmas \ref{lem2.4}, \ref{lem2.5}.
\end{proof}

Now we show that the second condition is also satisfied, this strictly depends on the value of $h(s)$ which in our model satisfies the same hypothesis that in $\cite{WIAS}$. This proof has been took from $\cite{WIAS}$ Proposition 6.
\begin{prop}
Let $\lambda: C\rightarrow [0,\infty)$ a locally Lipschitz  and bounded mapping. Furthermore, let $X^{(s)}$ be a Poisson point process on $sC$ with intensity function $\lambda^{(s)}$. Then the Condition \ref{con2} is satisfied for the directed spanning tree on $X^{(s)}$ together with the fluctuation function $h(s)=s^{1-1/(64d)}$.
\end{prop}
\begin{proof}
The proof will take several steps. Lets define the maximal radius of stabilization as $$R_{-,s}=\max_{X_{i}\in X^{(s)}:\pi_1(X_i)\leq 2s-s^{1/2d}}\vert X_i-\mathcal{A}(X_i)\vert $$ and let $E_s^{(1)}$ denote the event $\lbrace R_{-,s}\geq \frac{1}{6} s^{1/(2d)}\rbrace$.
\begin{lem}\label{lem2.6} As $s\rightarrow\infty$, $P(E^{(1)}_s)\in O(s^{-2d})$.
\end{lem}
\begin{proof}
Let $x\in sC$ with $2s-s^{1/(2d)}\leq \pi_1(x)$, since $X^{(s)}$ is a Poisson point process 

\begin{align}
P(\vert &x-\mathcal{A}(x, X^{(s)}\cup\lbrace x\rbrace)\vert >\frac{1}{6} s^{1/(2d)})\nonumber\\&\leq P(X^{(s)}\cap(B_{\frac{1}{6} s^{1/(2d)}}(x)\cap([\pi_1(x), \infty)\times \mathbb{R}^{d-1})=\emptyset)\nonumber\\
&\leq \exp(-\lambda_m \kappa_d 2^{-1}6^{-d}s^{1/2})\nonumber
\end{align}
\normalsize
And then by the Slyvniak-Mecke's theorem
\begin{align}
P(E^{(1)}_s)&=E(\mathds{1}\lbrace \exists X_i\in X^{(s)}:\vert X_i-\mathcal{A}(X_i, X^{(s)})\vert >\frac{1}{6} s^{1/(2d)}\rbrace) \nonumber\\
&\leq E(\sum_{X_i\in X^{(s)}}\mathds{1}\lbrace\vert X_i-\mathcal{A}(X_i, X^{(s)})\vert>\frac{1}{6} s^{1/(2d)}\rbrace) \nonumber\\
&\leq  \lambda_M s^d \eta_d(C)\exp(-\lambda_m \kappa_d 2^{-1}6^{-d}s^{1/2})\in O(s^{-2d}).\nonumber
\end{align}

\end{proof}
We write $$C_{-,s}=[-2s,2s-2s^{1/(2d)}]\times[-2s+2s^{1/(2d)},2s-2s^{1/(2d)}]^{d-1}.$$
Furthermore, we replace $s^{1/(2d)}$ by $s'$ and assume that $4s(s')^{-1}$ is an odd integer. This is no a restriction, since otherwise $s'$ can be adjusted in such a way that it is of the same order as $s^{1/(2d)}$, hence we can subdivide $sC$ on a cubic grid where each cube has center on $s'z$ and side $s'$.

Next, for $s\geq 1$ and $z\in \mathbb{Z}^{d}$ we let $H_{s,z}$ denote the event that the point processes $X^{(s)}$ and $X^{[\lambda_{s,z}]}$ agree on $Q_{3s'}(s'z)\cap sC$, where $$\lambda_{s,z}=\max\lbrace \lambda^{(s)}(x): x\in Q_{3s'}(s'z)\rbrace $$ denotes the maximum of the density $\lambda^{(s)}(\cdot)$ in the cube $Q_{3s'}(s'z)$ of side length $3s'$ centered at $s'z$. We say that the site $z$ (or the associated cube $Q_{3s'}(s'z)$) is \textit{$s$-good} if the event $H_{s,z}$ occurs. Sites that are not $s$-good are called $s$-bad.

We want to use that locally, trajectories do not deviate substantially from the horizontal line. More precisely, let $H'_{s,z}$ be the event that for every path $\Gamma (X_0)$ in the directed spanning tree on $X^{[\lambda_{s,z}]}\cap Q_{3s'}(s'z)$ such that $X_0$ is contained in $Q_{s'}(s'z)$ and whose endpoint $X_e$ satisfies $\pi_1(X_e)\leq \pi_1(s'z)+s'$ we have that $\Gamma(X_0)\subset Z^D_{(s')^{5/8}}$. We say that $E_s^{(2)}$ occurs if there exists $z\in \mathbb{Z}^d$ such that $s'z\in sC_{-,s}$ and $H'_{s,z}$ fails to occur. By \cite{Bac} Theorem 4.10 the next result follows.

\begin{lem} \label{lem2.7}
$P(E^{(2)}_s)\in O(s^{-2d})$ when $s$ tends to infinity.

\end{lem}

For any path $\Gamma$ in the directed spanning tree on $X^{(s)}$ we let
$$\#_s\Gamma=\#\lbrace z\in \mathbb{Z}^d: X_i\in Q_{s'}(s'z)\mbox{ for some }X_i \in \Gamma\rbrace $$ denote the number of $s'$-cubes intersected by $\Gamma$. Similarly, we let $\#_s\Gamma_{s,g}$ and $\#_s\Gamma_{s,b}$ denote the number of good and bad cubes that are intersected by $\Gamma$. We want to find an upper bound for the vertical displacement of a path $\Gamma$. If $\Gamma$ is a path in the directed spanning tree on $X^{(s)}$ starting from $X_0$ we let $$V(\Gamma)=\max_{X_i\in\Gamma}d(X_0, X_i) $$ where we write $d(X_0, X_i)=d_{\infty}(X_0+\mathbb{R}e_1, X_i)$, for the $d_\infty$ distance of $X_i$ to the horizontal line $X_0+\mathbb{R}e_1$.

\begin{lem}\label{lem2.8} Let $\Gamma\subset C_{-,s}$ be an arbitrary path in the directed spanning tree on $X^{(s)}$. Then  under the complement of the event $E^{(1)}_s\cup E^{(2)}_s$ almost surely, $$V(\Gamma)\leq 2s'+3(s')^{5/8}\#_{s,g}\Gamma+3s'\#_{s,b}\Gamma. $$
\end{lem}
\begin{proof}


We will proceed by induction under the number of vertex. The assertion is trivial if $V(\Gamma)\leq 2s'$, so we will assume that $V(\Gamma)>2s'$. Now lets proof the induction step. By shortening the path if necessary we may assume that the maximal displacement $V(\Gamma)$ is achieved at the endpoint $X_e$ of $\Gamma$.

\begin{figure}
\captionsetup{width=1\textwidth}
\centering
\includegraphics[width=1\textwidth]{induction.jpg}
\caption{Illustration of the induction step in the proof of Lemma \ref{lem2.8}. Figure took from \cite{WIAS}.}
\label{fig.2.4}
\end{figure}

Let $z_{0}\in \mathbb{Z}^d$ such that $Q_{s'}(s'z_0)$ contains the starting point $X_0$ of $\Gamma$. Then, we let $X_2\in\Gamma$ denote the first vertex of $\Gamma$ such that $X_2\in Q_{s'}(s'z_2)$ for some $z_2\in\mathbb{Z}^d$ with $d(s'z_0, s'z_2)>s'$. We also let $X_1\in\Gamma$ be such that $X_2=\mathcal{A}(X_1, X^{(s)})$. Similarly, we let $X_3\in\Gamma$ denote the last vertex of $\Gamma$ such that $X_3\in Q_{s'}(s'z_3)$ for some $z_3\in\mathbb{Z}^d$ with $d(s'z_0, s'z_3)\leq s'$. Finally, we put $X_4=\mathcal{A}(X_3, X^{(s)})$. This points can be constructed since we are under $(E_s^{(1)})^c$. Saw Figure \ref{fig.2.4} for an illustration of the construction.


We will denote the subpaths of $\Gamma$ from $X_0$ to $X_1$ and $X_4$ to $X_e$ as $\Gamma[X_0, X_1]$ and $\Gamma[X_4, X_e]$ respectively. Since $V(\Gamma)>2s'$ no $s'$-subcube is hit both $\Gamma[X_0, X_1]$ and $\Gamma[X_4, X_e]$, using this fact, Pythagoras' theorem and $(E_s^{(1)})^c$,
\begin{align}
V(\Gamma)&\leq V(\Gamma[X_0, X_2])+V(\Gamma[X_3, X_4])+V(\Gamma[X_4, X_e])\nonumber\\
&\leq 2s'+\vert X_3-X_4\vert+V(\Gamma[X_4, X_e])\leq \frac{5}{2}s'+V(\Gamma[X_4, X_e])\nonumber
\end{align}
Hence using the induction step, we arrive that
$$V(\Gamma)\leq \frac{5}{2}s'+2s'+3(s')^{5/8}\#_{s,g}\Gamma[X_4, X_e]+3s'\#_{s,b}\Gamma[X_4, X_e].$$
The assertion follows if $\Gamma[X_0, X_1]$ hits at least one $s$-bad cube. lets assume that $\Gamma[X_0, X_1]$ just intersects $s$-good cubes. Since $d(s'z_0, s'z_2)>s'$ and  $(E_s^{(1)}\cup E_s^{(2)})^c$ occurs, it follows that for $s'$ big enough $$\frac{5}{6}s'\leq d(X_0, X_1)\leq V(\Gamma [X_0, X_1])\leq (s')^{5/8}\#_{s,g}\Gamma[X_0, X_1]. $$
%The definition of the event E^2 is not very clear
And we complete the induction step by noting that
\begin{align}
V(\Gamma)&\leq \frac{5}{2}s'+2s'+3(s')^{5/8}\#_{s,g}\Gamma[X_4, X_e]+3s'\#_{s,b}\Gamma[X_4, X_e]\nonumber\\
&\leq 2s'+3(s')^{5/8}(\#_{s,g}\Gamma[X_0, X_1]+\#_{s,g}\Gamma[X_4, X_e])+3s'\#_{s,b}\Gamma[X_4, X_e]\nonumber\\
&\leq 2s'+3(s')^{5/8}\#_{s,g}\Gamma+3s'\#_{s,b}\Gamma\nonumber.
\end{align}
\end{proof}
We need to provide appropriate upper bounds on $\#_{s,g}\Gamma(X_i)$ and $\#_{s,b}\Gamma(X_i)$ for any $X_i\in X^{(s)}$.

\begin{lem}\label{lem2.9}

\textbf{1.} For every $z\in\mathbb{Z}^d$ with $s'z\in C_{-,s}$ it holds that almost surely $$P(H_{s,z}^c\vert X^{(s)})\leq L \sqrt{d}3^{d+1}s^{-1/4}$$ where $L$ is the global Lipschitz constant of $\lambda$. 

\textbf{2.} Every path $\Gamma\subset C_{-,s}$ in the directed spanning three on $X^{(s)}$ almost surely satisfies that
$$\mathds{1}\lbrace \#_s\Gamma\leq 3^{d+3}s(s')^{-1}\rbrace P(\#_{s,b}\Gamma\geq  s^{1-5/(8d)}\vert X^{(s)})\leq\exp (-\sqrt{s}).$$
\end{lem}
\begin{proof}

\textbf{1.} The event $H_{s,z}$ is expressed as $$X  \cap\lbrace (X_i, U_i): X_i \in Q_{3s'}(s'z)\mbox{ and }\lambda^{(s)}(X_i)\leq U_i \leq\lambda_{s,z}\rbrace=\emptyset .$$
Hence $$H_{s,z}^{c}\subset\lbrace (X_i, U_i): X_i \in Q_{3s'}(s'z)\mbox{ and }\lambda^{(s)}_{z,m}\leq U_{i}\leq\lambda_{s,z}\rbrace,$$ with $\lambda_{s, z, m}=\min\lbrace \lambda^{(s)}(x):x\in Q_{3s'}(s'z)\rbrace$, i.e. a Poisson point process with intensity $\lambda_{s,z}-\lambda_{s, z, m}$. As a consequence if we condition $H_{s,z}^c$ on $X^{(s)}$ the number of points in $P(H_{s,z}^c\vert X^{(s)})$ are bounded by a Poisson random variable of parameter $$(\lambda_{s,z}-\lambda_{s, z, m})\eta_d( Q_{3s'}(s'z))=(\lambda_{s,z}-\lambda_{s, z, m})3^ds^{1/2}. $$ Since $\lambda$ is locally Lipschitz, and the longest diagonal of $Q_{3s'}(s'z)$ has length $\sqrt{d}3s^{-1/2}$, we obtain that almost surely
$$P(H_{s,z}^c\vert X^{(s)})\leq L\sqrt{d}3^{d+1}s^{-1/2+1/(2d)}.$$
We finish the first inequality  observing that $-1/2+1/(2d)\leq -1/4$.

\textbf{2.} Let $\gamma=\lbrace z\in\mathbb{Z}^d: Q_{s'}(s'z)\cap\Gamma\neq\emptyset\rbrace$ be the discretization of $\Gamma$ and define $M_i=z_i+3\mathbb{Z}^d$, where $z_i\in\mathbb{Z}^d$ can be chosen such that $\lbrace M_1, ...,M_K\rbrace$ is a partition of $\mathbb{Z}^d$ of $K=3^d$.

For every $i$ conditioned on $X^{(s)}$ the process of $s$-good sites is an independent site process on $\gamma_i=\gamma\cap M_i$. By 1. conditioned on $X^{(s)}$ the probability for a site to be $s$-bad is of order $O(s^{-\frac{1}{4}})$. Let $\#_b \gamma$ denote the number of bad sites in $\gamma$ then by the Binomial concentration inequality (\cite{Pen} Lemma 1.1) under the event $\lbrace \#\Gamma\leq 3^{d+1}s(s')^{-1}\rbrace$ we have that
$$P(\#_{s,b}\Gamma\geq  s^{1-5/(8d)}\vert X^{(s)})\leq \sum_{i=1}^K P(\#_b \gamma_i \geq K^{-1}s^{1-5/(8d)})\leq\exp (-\sqrt{s}).$$
\end{proof}

We proceed to proof our last auxiliary lemma for that let $E^{(3)}_s$ denote the event that there exists a point $X_i\in X^{(s)}$ such that $$\#_s\Gamma_{-,s}(X_i)\geq 3^{d+3}s(s')^{-1},$$ where $\Gamma_{-,s}(X_i)$ denote the longest subpath of $\Gamma (X_i)$ that starts at $X_i$ and is contained in $C_{-,s}$.

\begin{lem}\label{lem2.10}

\textbf{1.} Let $E^{(4)}_s$ the event that exists a finite connected set $\gamma\in\mathbb{Z}^d$ such that
\begin{itemize}
\item $\#\gamma\geq \sqrt{s}$,
\item $s'z\in D_{-,s}$ holds for every $z\in\gamma$,
\item the number of $s$-good sites intersected by $\gamma$ is at most $\#\gamma/2$.
\end{itemize}
Then $\lim_{s\rightarrow\infty}P(E_s^{(4)})=0.$

\textbf{2.}Suppose that $(E_s^{(1)}\cup E_s^{(2)})^c$ occurs and let $X_i\in X^{(s)}$ be arbitrary. Furthermore, let $X_e\in X^{(s)}$ be the end point of $\Gamma_{-,s}(X_i)$. Then $$\pi_1(x)(X_e-X_i)\geq 3^{-d-2}s'\#_{s,g}\Gamma_{-,s}(X_i).$$

\textbf{3. } $P(E^{(3)}_s)\in O(s^{-2d})$ when $s$ tends to infinity.

\end{lem}
\begin{proof}

\textbf{1.} The process of $s$- good sites is dominated by a Bernoulli site percolation process with probability $p\in (0,1)$ due two \cite{Lig} Theorem 0.0 which hypothesis are satisfied by Lemma \ref{lem2.9} 1. For $s$ sufficiently large $p$ can be chosen arbitrarily close to $1$.

We have that for a fixed connected set the probability that $\gamma$ contains at least $\#/2$ bad sites is at most $2^{\#\gamma/2}(1-p)^{\#\gamma/2}$. Also by \cite{Pen} Lemma 9.3 the number of connected sets containing $k\geq 1$ sites is bounded above by $s^d 2^{3^d k}$. Therefore 
$$P(E_s^{(4)})\leq s^d\sum_{k\geq \sqrt{s}}2^{3^d k}2^k(1-p)^{k/2}, $$
which is of order $O(s^{-2d})$, since $p$ can be chosen sufficiently close to $1$.

\textbf{2.} Let $\gamma$ be a subset of $s$- good sites whose cubes are intersected by $\Gamma_{-,s}(X_i)$ such that every pair of distinct sites in $\gamma$ is at least a three cubes of distance and $\# \gamma\geq 3^{-d}\#_{s,g}\Gamma_{-,s}(X_i)$. Writing $\gamma=\lbrace z_1, ...,z_k\rbrace$ we define $\Gamma_1, ...,\Gamma_k$ sub paths of $\Gamma$, where the starting point $X_{j,0}$ of $\Gamma_j$ is the first point of $\Gamma$ that is contained in $Q_{s'}(s'z_j)$. Starting from $X_{j,0}$, $\Gamma_j$ is the longest subpath of $\Gamma$ that is contained in the left half-space $(-\infty, \pi_1(s'z_j)+s')\times\mathbb{R}^d$.

Since the events $E_s^{(1)}$ and $E_s^{(2)}$ do not occur these sub paths are disjoint and $\pi_{1}(X_{j,e}-X_{j,0})\geq s'/6$ where $X_{j,e}$ denotes the endpoint of $\Gamma_j$. Combining these lower bounds shows that $$\pi_{1}(X_{e}-X_{i})\geq \frac{s'}{6}\#\gamma\geq 3^{-d-2}s'\#_{s,g}\Gamma_{-,s}(X_i).$$

\textbf{3.} If we show that $E_s^{(3)}\subset E_s^{(1)}\cup E_s^{(2)}\cup E_s^{(4)}$ the result follows by Lemmas \ref{lem2.6}, \ref{lem2.7} and 1. on this lemma. Assume that $E_s^{(3)}$ and $(E_s^{(1)}\cup E_s^{(2)}\cup E_s^{(4)})^c$ occur simultaneously, we will derive a contradiction. Then there exists\\ $X_i\in X^{(s)}$ such that $\#_{s}\Gamma_{-,s}(X_i)\geq 3^{d+3}2s(s')^{-1}$. Since $(E_s^{(4)})^c$ occurs, we obtain that
$$\#_{s,g}\Gamma_{-,s}(X_i)\geq\frac{1}{2}\#_{s}\Gamma_{-.s}(X_i)\geq 3^{d+2}s(s')^{-1}. $$

In particular 2. on this lemma would imply that $\pi_1(X_e-X_i)\geq 2s$. But this is impossible since both $X_1$ and $X_e$ are contained in $sC$ an open hypercube of side length $2s$.

And we are ready to proof the proposition.
\end{proof}
\textit{Proof of Proposition.} We can bound $$P((E^D_s)^c)\leq P((E^D_{s, \varepsilon, 1})^c)+P((E^D_{s, \varepsilon, 2})^c).$$

For the first summand lets observe that we can estimate
$$ (E^D_{s, \varepsilon, 1})^c \subset  \lbrace \#( X^{(s)}\cap (sC)_{\varepsilon s})\leq 1\rbrace$$
As since $X^{(s)}$ is a Poisson point process 
$$P(\#( X^{(s)}\cap (sC)_{\varepsilon s})\leq 1)=e^{-\lambda_m s^d\eta_d( C_{\varepsilon})}(1+s^d\lambda_M\eta_d( C_{\varepsilon})). $$ Which implies that $P((E^D_{s, \varepsilon, 1})^c)\in O(s^{-2d})$.

Lets proceed to bound the second summand. First we write the event 
$$E^{D_-}_{s, \varepsilon, 2}=\lbrace \Gamma_{-, s}(X_i)\subset Z^D_{h(s)}(X_i)\mbox{ for all } X_i\in X^{(s)} \rbrace, $$ we claim that $$(E^{D}_{s, \varepsilon, 2})^c\subset (E^{D,-}_{s, \varepsilon, 2})^c\cup E_s^{(1)},$$ which is equivalent to show that $E^{D_-}_{s, \varepsilon, 2}\cap (E_s^{(1)})^c\subset E^D_{s, \varepsilon, 2}$. Then lets assume that we are under the event $E^{D_-}_{s, \varepsilon, 2}\cap (E_s^{(1)})^c$.
Let $X_i\in X^{(s)}$ and lets assume that $s$ is big enough that $(sC)_{\varepsilon s}\subset C_{-,s}$. We have two cases

\textbf{Case 1) $\Gamma(X_i)\subset C_{-,s}$}. Hence $\Gamma(X_i)=\Gamma_{-, s}(X_i)\subset Z_{h(s)}^D(X_i)$ and since $(sC)_{\varepsilon s}\subset C_{-,s}$ then $\bar{\Gamma}(X_i)\cap (sC)_{\varepsilon s}\subset Z_{h(s)}^D(X_i)$.
\begin{figure}
\captionsetup{width=1\textwidth}
\centering
\includegraphics[width=.7\textwidth]{cons.jpg}
\caption{A not possible path under $E^{D_-}_{s, \varepsilon, 2}\cap (E_s^{(1)})^c$. Figure took from \cite{WIAS}.}
\label{fig.2.5}
\end{figure}

\textbf{Case 2) $\Gamma(X_i) \nsubseteq C_{-,s}$}. We will proof that $\Gamma (X_i)\cap (sC)_{\varepsilon s}=\emptyset$ which implies that $\bar{\Gamma}(X_i)\cap (sD)_{\varepsilon s}\subset Z_{h(s)}^D(X_i)$ then without lost of generality we can  assume that $X_i\in sC\setminus C_{-,s}$. Lets assume that $\Gamma (X_i)\cap (sC)_{\varepsilon s}\neq\emptyset$ and let $X_{i_2}\in\Gamma (X_i)\cap (sC)_{\varepsilon s}$. Since we are on the event $(E_s^{(1)})^c$ for $s$ big enough it implies that exists $X_{i_1}\in \Gamma (X_i)$ such that  $d(X_{i_1}, X_{i_2})\geq h(s)$ which contradicts the occurrence of $E^{D_-}_{s, \varepsilon, 2}$ and the claim is finished. The construction is illustrated in Figure \ref{fig.2.5}.


Now that we proved that $(E^D_{s, \varepsilon, 2})^c\subset (E^{D_-}_{s, \varepsilon, 2})^c\cup (E_s^{(1)})$ by Lemma \ref{lem2.6} it remains to show that $P((E^D_{s, \varepsilon, 2})^c\in O(s^{-2d})$. For $X_i\in X^{(s)}\cap C_{-,s}$ such that $\Gamma_{-,s}(X_i)\nsubseteq Z^D_{s^{1-1/(64d)}}(X_i)$ then  $V(\Gamma_{-,s}(X_i))\geq s^{1-1/(32d)}$. Therefore by Lemma \ref{lem2.8} after some technical computations yields that
\small
\begin{align}
P(&(E^D_{s, \varepsilon, 2})^c)\nonumber\leq P(\sup_{X_i\in X^{(s)}\cap C_{-,s}} \#_{s,b}\Gamma_{-,s}(X_i)\geq s^{1-1/(32d)})\\
&\leq P(E_s^{(1)}\cup E_s^{(2)}\cup E_s^{(3)})+P((E_s^{(3)})^c\cap \lbrace\sup_{X_i\in X^{(s)}\cap C_{-,s}} \#_{s,b}\Gamma_{-,s}(X_i)\geq s^{1-1/(32d)}\rbrace)\nonumber.
\end{align}
\normalsize
By Lemmas \ref{lem2.6}, \ref{lem2.7} and \ref{lem2.10} the first summand is of order $O(s^{-2d})$. For the second we may condition on $X^{(s)}$ so by Lemma \ref{lem2.9} and Campbell's theorem

\begin{align}
P(&(E_s^{(3)})^c\cap\lbrace\#_{s,b}\Gamma_{-,s}(X_i)\geq s^{1-9/(16d)}\mbox{ for some }X_i\in X^{(s)}\cap C_{-,s}\rbrace)\nonumber\\
&\leq E(\mathds{1}\lbrace (E_s^{(3)})^c\rbrace\sum_{X_i\in X^{(s)}\cap D_{-,s}}P(\#_{s,b}\Gamma_{-,s}(X_i)\geq  s^{1-5/(8d)}\vert X^{(s)}))\nonumber\\
&\leq\exp (-\sqrt{s})E\#X^{(s)}\leq\exp (-\sqrt{s})\lambda_M s^d\eta_d(C),\nonumber
\end{align}
which is of order $O(s^{-2d})$.
\end{proof}

