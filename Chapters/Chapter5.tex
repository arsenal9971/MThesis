\chapter{Conclusion and outlook}

We presented a complete light field processing pipeline, from sparse acquisition and point tracking (geometry extraction) to reconstruction and depth map computation. Our approach on tackling this problem is centered in the complexity reduction of already known light field recovery methods as well as the development of a transparent open source method that can be implemented in almost any personal computer.

\section{Recapitulation}

We began by presenting an historical review of the light field photography as well as the different approaches on the acquisition of the light field that are related to distinct proposed methods. We also presented the advantages and disadvantages of this methods and we exposed the reason of our choice of the acquisition methods as a sequence of views of a scene captures by a digital camera moving in straight line on a mechanical track. We explained the geometrical proxy needed by this approach, i.e.\ the Epipolar Geometry that can be tought as a generalization of Stereo Vision with more than two views, were one assume that the views follow a stright line trajectory which reduces the complexity of tracking of the feature points on the scene. At the same time we introduced the followed point tracking algorithm, knwon as the Lucas-Kanade method that let us track the $N$-strongest corners in the image, in the same sense we pointed out that with this approach the method has poor performance when trying to estimate the light field of scenes that are poor in number of corners; we also presented the code that we used to implement the point tracking that let us sketch the sparse sampled Epipolar plane images. We also found out that the best maximum disparity between consecutive images in order to have a trustworthy reconstruction is $d_{max}=7$ pixeles; this disparity was used in our method.

\bigskip 

Having already the Epipolar Plane Images corresponding to our sparse sampled light field (obtained from a sequence of scenes acquired by Professor Markus Gross from the Disney Research Center based on Zurich), we conclude based on the work of Professor Suren Vagharshakyan and his team \cite{LF-Shearlets} than a way to recover a dense sampled light field and then compute the depth map of the scene one could use the inpainting algorithm based on the sparsifying system generated by $0$-Shearlets which are obtained by selecting as scaling sequence the parameters $\alpha_j=-2/j$, such system is sensible to straight-line structures as the one that are obtained in the Epipolar Plane Images related to a sequence of views taken with a straight line trajectory; we used theory on the Universal Shearlet Systems to prove that in fact the $0$-Shearlets system forms a Parseval frame and therefore capable to inpaint the sparse EPIs. 

\bigskip

The chosen method to perform the inpainting-based reconstruction of the sparse EPIs was the Iterative Hard Thresholding (this choice is a classic on sparse reconstruction methods) using the Shearlets system as sparsifying system. Based on the work of Thomas Blumensath explained on his paper \cite{hard-thresholding} we choose to use an acceleration term presented on the Algorithm~\ref{alg:lfshearlets1} which improves the convergence time of the algorithm in comparison of the former methods that uses the classic iterative hard thresholding (see \cite{clustered-inpainting}). We obtained inpainted EPIs present an acceptable quality in the sense that all the lines corresponding to the different tracked feature points are clear, therefore it is possible to have a good estimation of the depth map of the scene. To implement the reconstruction method we decided to use the julia implementation of the Shearlet toolbox Shearlab3D that was developed by the author of this thesis; this decision was made based on the performance benchmarks that position this implementation as currently the fastest on the market. The code used to perform the reconstruction is also fully presented on this thesis.

\bigskip

Finally, having the inpainted EPIs the only thing left to do in order to estimate the depth map computing the slope of the different lines on the EPIs that will give us the depth of the associated feature points, for this first we needed to detect the lines; by its effectiveness and simplicity we picked the Hough line transform to perform this task, using the python OpenCV API to implement it. Comparing the obtained depth map with the given depth map corresponding to our dataset computed by Markus Gross and his group on Zurich, we can finally conclude that our approach performs very well and it can be taken as a good method to produce trustfull depth map estimation of static scenes, this was exactly the answer we were looking for. In addition when comparing the running time and used hardware of our method with the other works on the same topic, we can also conclude that our method is faster than the others and it also requires less powerful hardware, since it can be executed in a common personal computer while on the other related works cluster-servers with several processing cores were used with running times from one to ten hours, of course our method is not perfect and its downsides reside on the low quality of the inpainted EPIs since the used point tracking method can track just strong corners that are usually not many, in this sense as mentioned before this method will be more effective in scenes dominated by corners and almost useless in scenes without corners.

\section{Future work}

As we mentioned in the last section, the method that we presented in this thesis has some limitations in terms of the number of points that can be tracked and then from which the depth can be computed. This method open up several opportunities for improvements and future research; some of them are listed bellow:

\begin{itemize}
\item \textbf{More sensible tracking algorithm:} As we already mentioned before, the point tracking algorithm that we used is designed to track strong corners detected by the Shi-Tomasi method; when the scene doesn't have a high number of corners in each object. It would be very convinient to use an algorithm that has a higher sensitivity of different points; to tackle this problem we propose two possible solutions: One could use the Gunnar Farneb\"ack's algorithm for dense optical flow proposed by the computer scientist Gunnar Fanerb\"ack in 2003\cite{Gunnar}; this algorithm is already implemented in OpenCV but the application is more complicated and slow than the Lucas-Kanade algorithm. The other solution is based in the limitation than the Lucas-Kanade algorithm just tracks strong corners, so objects that have very smooth boundaries are hard to track, one could then rough the edges of the image this can be performed by decreasing the resolution of the image; this is a trade-off since decreasing the resolution will decrease the number of overall points in the image but it also might increase the number of points easy to track. 

\item \textbf{GPU-paralelization of different steps in the pipeline:} In our pipeline the only step that is already paralelized in gpu is the Shearlet transform since Shearlab.jl has also the option of use the graphic processor to accelerate the generation of the Shearlet system as well as the decoding and encoding. The point tracking is easy to paralelize since one can perform the optical flow of the feature points individually in the gpu, OpenCV 3.0 has an option to peform Lucas-Kanade with Nvidia CUDA (see \url{http://docs.opencv.org/3.0-beta/modules/cudaoptflow/doc/optflow.html}). The step that is not so trivial to paralelize is the reconstruction algorithm, i.e.\ the inpainting by Iterative Hard Thresholding; Jared Tanner in 2010 proposed a method to paralelize Large Scale Iterative Hard Thersholding on gpu's \cite{Tanner}, although the code is not open source yet one could try to implement it in some high-level language like julia.

\item \textbf{Open source and cheap hardware design of a light field camera using the method:} In this thesis we used a data set of views taken by the Disney Research Group at Zurich, their acquisition setup consists of a digital camera; a personal computer and a linear mechanical track, this represents a very expensive, bulky and heavy hardware that clearly cannot be hand-held. We are currently developing a hand-held light field camera using the same idea of the original setup but in an smaller scale. This camera consists in a small single-board computer Raspberry Pi 3 with python and julia installed, a plastic case, an external battery and touchscreen and Raspberry Pi camera module (5 Mp digital camera) that moves on an small electronic linear track; this setup can be hand-held, is very light and cheap (approximately 70 euros) and most importantly, is open hardware, so anybody can build it at home, modify it and improve it. 

\item \textbf{Rendering of the obtained depth map:} In this thesis we estimated the depth map of a static scene, but this just give us a set of depths and the interesting part of the light field theory is when one apply this estimated depths to render the actual light field, like implementing the dynamic digital focus in a camera and then be able to focus any feature in the object after the picture was already taken, this can be perform by the combination of a contour detector and a gaussian filter to apply the bokeh effect that defocus the objects that have different depth than the feature of the corresponding contour, this process is explained in detail in Ren Ng's PhD Dissertion\cite{RenNg} which originated the commercial light field camera's company Lytro. One can also generate a 3D visualization of the scene for Virtual Reality applications, although this needs a deep understanding of 3D graphics visualization that for now we lack; Marc Levoy and Pat Hanrahan from Stanford University explain with detail this process in their paper "Light Field Rendering"\cite{LF-rendering}.

\end{itemize}
