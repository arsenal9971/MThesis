\chapter{Conclusion and outlook}

We presented a complete light field processing pipeline, from sparse acquisition and point tracking (geometry extraction) to reconstruction and depth map computation. Our approach on tackling this problem is centered in the complexity reduction of already known light field recovery methods as well as the development of a transparent open source method that can be implemented in almost any personal computer.

\section{Recapitulation}

We began by presenting an historical review of the light field photography as well as the different approaches on the acquisition of the light field that are related to distinct proposed methods. We also presented the advantages and disadvantages of this methods and we exposed the reason of our choice of the acquisition methods as a sequence of views of a scene captures by a digital camera moving in straight line on a mechanical track. We explained the geometrical proxy needed by this approach, i.e.\ the Epipolar Geometry that can be tought as a generalization of Stereo Vision with more than two views, were one assume that the views follow a stright line trajectory which reduces the complexity of tracking of the feature points on the scene. At the same time we introduced the followed point tracking algorithm, knwon as the Lucas-Kanade method that let us track the $N$-strongest corners in the image, in the same sense we pointed out that with this approach the method has poor performance when trying to estimate the light field of scenes that are poor in number of corners; we also presented the code that we used to implement the point tracking that let us sketch the sparse sampled Epipolar plane images. We also found out that the best maximum disparity between consecutive images in order to have a trustworthy reconstruction is $d_{max}=7$ pixeles; this disparity was used in our method.

\bigskip 

Having already the Epipolar Plane Images corresponding to our sparse sampled light field (obtained from a sequence of scenes acquired by Professor Markus Gross from the Disney Research Center based on Zurich), we conclude based on the work of Professor Suren Vagharshakyan and his team \cite{LF-Shearlets} than a way to recover a dense sampled light field and then compute the depth map of the scene one could use the inpainting algorithm based on the sparsifying system generated by $0$-Shearlets which are obtained by selecting as scaling sequence the parameters $\alpha_j=-2/j$, such system is sensible to straight-line structures as the one that are obtained in the Epipolar Plane Images related to a sequence of views taken with a straight line trajectory; we used theory on the Universal Shearlet Systems to prove that in fact the $0$-Shearlets system forms a Parseval frame and therefore capable to inpaint the sparse EPIs. 

\bigskip

The chosen method to perform the inpainting-based reconstruction of the sparse EPIs was the Iterative Hard Thresholding (this choice is a classic on sparse reconstruction methods) using the Shearlets system as sparsifying system. Based on the work of Thomas Blumensath explained on his paper \cite{hard-thresholding} we choose to use an acceleration term presented on the Algorithm~\ref{alg:lfshearlets1} which improves the convergence time of the algorithm in comparison of the former methods that uses the classic iterative hard thresholding (see \cite{clustered-inpainting}). We obtained inpainted EPIs present an acceptable quality in the sense that all the lines corresponding to the different tracked feature points are clear, therefore it is possible to have a good estimation of the depth map of the scene. To implement the reconstruction method we decided to use the julia implementation of the Shearlet toolbox Shearlab3D that was developed by the author of this thesis; this decision was made based on the performance benchmarks that position this implementation as currently the fastest on the market. The code used to perform the reconstruction is also fully presented on this thesis.

\bigskip

Finally, having the inpainted EPIs the only thing left to do in order to estimate the depth map computing the slope of the different lines on the EPIs that will give us the depth of the associated feature points, for this first we needed to detect the lines; by its effectiveness and simplicity we picked the Hough line transform to perform this task, using the python OpenCV API to implement it. Comparing the obtained depth map with the given depth map corresponding to our dataset computed by Markus Gross and his group on Zurich, we can finally conclude that our approach performs very well and it can be taken as a good method to produce trustfull depth map estimation of static scenes, this was exactly the answer we were looking for. In addition when comparing the running time and used hardware of our method with the other works on the same topic, we can also conclude that our method is faster than the others and it also requires less powerful hardware, since it can be executed in a common personal computer while on the other related works cluster-servers with several processing cores were used with running times from one to ten hours, of course our method is not perfect and its downsides reside on the low quality of the inpainted EPIs since the used point tracking method can track just strong corners that are usually not many, in this sense as mentioned before this method will be more effective in scenes dominated by corners and almost useless in scenes without corners.

\section{Future work}

As we mentioned in the last section, the method that we presented in this thesis has some limitations in terms of the number of points that can be tracked and then from which the depth can be computed. 
