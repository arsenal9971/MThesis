\chapter{Information velocity in the SINR graph}
For finishing this work we present a telecommunication model. Every point of our process will represent an user that want to sent or receive a message. We want to present a sending strategy that satisfies two goals

1. Every message is send  from one user to another in finite time,

2. The message travels around the network with positive velocity.

For achieving it we will use a power control policy and a conic forwarding strategy. The two main results of this chapter are taken from \cite{Iye} where is considered the case of a stationary Poisson point process. Using coupling techniques we will extend the first result to the non stationary case.

\section{SINR graph, power control policy and conic forwarding strategy}

In this chapter $X$ will be a point process and every $x\in X$ will represent a user of a network that want to transmit and receive information at time $t$. We will denote by $X_{T}(t)$, $X_R(t)$ the users that are transmitting and receiving messages at time $t$ respectively, this sets are disjoint and satisfy $X_{T}(t)\cup X_R(t)=X$. The Signal Interference Noise plus Radio measures the quality of the communication from a user $x$ to a user $y$ at time $t$.
\begin{defn} We define the SINR from $x\in X_T (t)$ to a $y\in X_R (t)$ as
$$SINR_{xy} (t)=\frac{P_x(t)h_t(x,y)\ell(x,y)}{\gamma I(t)+N}. $$
Where at time $t\in\mathbb{Z}^+$,
\begin{itemize}
 \item $P_x(t)$ is the power of transmission  of $x$,
 \item $h_t(x,y)$ are the space-time fading coefficients from $x$ to $y$,
 \item $\ell(x,y)$ is the path-loss function which measures the decaying of the quality of the transmission according to the distance between $x$ and $y$,
 \item $I(t)=\sum_{z\in X_T(t)\setminus\lbrace x,y \rbrace}P_z(t)h_t(z,y)\ell(z,y)$ is the interference coming from the other nodes,
 \item $\gamma$ is a constant and $N$ a Gaussian noise,
\end{itemize}
 and all are positive functions.

We say that the transmission from $x\in X_T (t)$ to $y\in X_R (t)$ has been successful if $SINR_{xy}>\beta$ a positive constant. 
\end{defn}

At all times the location of the users of the network will be fixed. If at time zero the transmission from $x$ to $y$ is not successful, at time one a new independent trial is done, and the process continue until the message is sent. Then once it arrives to $y$ a new node $z$ is picked and the process restart again from $y$ to $z$. We will propose a model where the expected value of the delay time of successfully transmitting one message from a user to another is finite and the average velocity in which the message travels trough the network is strictly positive. For that we will assume that
\begin{itemize}
  \item $X$ is a Poisson point process in $\mathbb{R}^2$, 
  \item ${h_{t}(x,y),x,y\in X,t=0,1,...}$ are independent exponential random variables of parameter $\mu$,
  \item $\ell(x,y)=1\wedge\vert x-y\vert^{-\alpha} $ for $\alpha>2$,
  \item every node is a transmitter or a receiver at time $t$ following a Bernoulli random variable $\textbf{1}_x(t)$ with success probability $p_x(t)$,
  \item $0<\gamma<1$,
  \item $P_x(t)=c\ell(x,y)^{-1}$ where $c=M(1-\varepsilon)^{-1}$, $0<\varepsilon<1$,
  \item $M=P_x(t)p_x(t).$
\end{itemize}
The last two assumptions is what is called \textit{power control policy}. Then we can rewrite 
$$SINR_{xy} (t)=\frac{P_x(t)h_t(x,y)\ell(x,y)\textbf{1}_x(t)(1-\textbf{1}_y(t))}{\gamma I(t)+N}. $$
The next random variable tell us if the transmission at time $t$ has been successful
\begin{displaymath}
e_{xy}(t)= \left\{ \begin{array}{ll}
1 & \textrm{1 if $SINR_{xy}>\beta$}\\
0 & \textrm{otherwise}.
\end{array} \right.
\end{displaymath}

\begin{figure}
\captionsetup{width=1\textwidth}
  \centering
    \includegraphics[width=0.6\textwidth]{cone}
    \caption{Definition of cones with angles $2\phi$. Figure taken from \cite{Iye}.}
    \label{figcone}
\end{figure}
\begin{defn} The space-time SINR graph is a graph with vertex on $X\times\mathbb{Z}^{+}$ where a directed edge exists from $(x,t)$ to $(y,t+1)$ if $e_{xy}(t)=1$. Since given $X$ the location of the nodes does not change on time, the evolution of the graph is due to the changes in the fading variables $h_t(x,y)$ and $X_T$.
\end{defn}



For finishing our model we have to specify how does a user $x$ choose a user $y$ to transmit the message. The network will use a conic forwarding strategy. For that let $C_1,...,C_m$ be cones centered in the origin with angle $2\phi<\frac{\pi}{2}$ such that. $\cup_{i=1}^m C_i=\mathbb{R}^2$ and are disjoint, also lets assume that $C_1$ is symmetric with respect to the x axis (see Figure \ref{figcone}). 

At time $t$ the user $x$ will transmit through the cone $x+C_d(x,t)$ that contains the final destination of the package. It will choose the nearest user inside that cone which we will denote by $n_t(x)$ (see Figure \ref{neighb}). 
\begin{figure}
\captionsetup{width=1\textwidth}
  \centering
    \includegraphics[width=0.6\textwidth]{neighb}
    \caption{Each node transmits to its nearest neighbor in the destination cone.}
 \label{neighb}
\end{figure}
\section{Exiting time from the destination cone} 

Using similar techniques to \cite{Iye} Theorem 3.2 where it is analyzed the case of a stationary Poisson point process we will proof that following the conic forwarding strategy a message can be transmitted in finite time from one user to another in the non stationary case.. Let the minimum exit time taken by any packet to be successfully transmitted from node $x_o$ to its nearest neighbor $n(x_o)$ in the destination cone of the packet be

$$T_{x_o}=\min\lbrace t>0:e_{x_o,n_t(x_o)}(t)=1\rbrace. $$


\begin{thm}\label{teo4.1} Let $X$ be a Poisson point process with bounded intensity function $0<\lambda_m\leq\lambda(x)\leq\lambda_M$. If $\beta\gamma<1$ then the SINR graph with power control policy and conic following strategy satisfies that $E(T_{x_o})<\infty$ for any $x_o\in X$.
\end{thm}
\begin{proof}

Lets assume that the package is sent by a fix user $x_o\in X$, let $C_d$ the destination cone of this package and $n(x_o)$ the nearest neighbor of $x_o$ in $C_d$.
%It is important to remark that until the package is not transmited n(o) is fixed

We have that 
$$P(T_{x_o}>k\mid X)=E\lbrace\prod_{t=1}^k P(A(t)\cup B(t)\cup C(t)\mid \mathcal{G}_k)\textbf{1}_F\vert X \rbrace.$$
Where
\begin{itemize}
\item$F=\cap_{j=2}^{k}\lbrace p_{x_o}(j)=p_{x_o}(1)\rbrace$,
\item$\mathcal{G}_{k}$ is the $\sigma$-algebra generated by $X$ and the choice of the cones made at all nodes of $X$ up to time $k$,
\item$A(t)=\lbrace x_o\in X_R(t)\rbrace$,
\item$B(t)=\lbrace x_o\in X_T(t), n(x_o)\in x_R(t), SINR_{x_o, n(x_o)}(t)\leq\beta\rbrace$,
\item$C(t)=\lbrace x_o\in X_T(t), n(x_o)\in X_T(t)\rbrace$.
\end{itemize}


On the event $F$ since $c=P_{x_o}(t)\ell(x_o,n(x_o))$ and $h_t(x_o,n(x_o))$ has exponential distribution of parameter $\mu$,
\begin{align*}
P(A(t)\vert\mathcal{G}_k)&=1-p_{x_o}(1),\\
P(B(t)\mid\mathcal{G}_k)&=p_{x_o}(1)q_{n(x_o)}(t)P(SINR_{x_o,n (x_o)}\leq \beta\vert \mathcal{G}_k)\\
&=p_{x_o}(1)q_{n(x_o)}(1)P(h_t(x_o,n(x_o))\leq\frac{\beta(N+\gamma I(t))}{c}\vert\mathcal{G}_k\rbrace)\\
&=p_{x_o}(1)q_{n(x_o)}(1)(1-E\lbrace e^{-\frac{\mu\beta}{c}(N+\gamma I(t))}\vert\mathcal{G}_k\rbrace)\\
P(C(t)\vert\mathcal{G}_k)&=p_{x_o}(1)(1-q_{n(x_o)}(1)).
\end{align*}
Hence since $ q_{n(x_o)}(1)=1-\ell(x_o, n(x_o))(1-\varepsilon)\geq \varepsilon$, we can estimate
\begin{align}
P(A(t)\cup B(t)\cup C(t)\vert\mathcal{G}_k)\leq 1-p_{x_o}(1)\varepsilon e^{-\frac{\mu\beta N}{c}}E(e^{-\frac{\mu\beta \gamma}{c}I(t)}\vert\mathcal{G}_k). \label{eq:1}
\end{align}

Suppose that $x\in X\setminus\lbrace x_o,n(x_o)\rbrace$ transmits using the cone $x+C_i$ at time $t$ with transmission probability $p_x^{(i)}(t)$ and power $P_x^{(i)}(t)$. Then we have that for $a=\frac{\mu\beta\gamma}{c}$
$$E\lbrace e^{-\frac{\mu\beta\gamma}{c}I(t)}\vert\mathcal{G}_k\rbrace=\prod_{x\in X\setminus\lbrace x_o,n(x_o)\rbrace}E\lbrace e^{-a\textbf{1}_x P_x^{(i)}(t)h_t(x,n(x_o))\ell(x,n(x_o))}\vert\mathcal{G}_k\rbrace.$$ And again from the fact that $h_t(x,n(x_o))$ has exponential distribution of parameter $\mu$ and by the power control policy,
\begin{align}
E\lbrace & e^{-a\textbf{1}_x P_x^{(i)}(t)h_t(x,n(x_o))\ell(x,n(x_o))}\vert\mathcal{G}_k\rbrace\nonumber\\
&=(1-p_x^{(i)})+p_x^{(i)}E\lbrace e^{-aP_x^{(i)}(t)h_t(x,n(x_o))\ell(x,n(x_o))}\vert X\rbrace\nonumber\\
&=(1-p_x^{(i)})+p_x^{(i)}\frac{c}{c+\beta\gamma\ell(x,n(x_o))P_x^{(i)}(t)}.\label{eq:2}
\end{align}
%MGF of an exponential

For $x\in X\setminus\lbrace x_o,n(x_o)\rbrace$ let $C_{x}^{*}=C_{x}^{*}(X)$ the cone which minimizes (\ref{eq:2}) and in consequence maximizes (\ref{eq:1}), $p^{*}_x$, $P^{*}_x$, $\textbf{1}^{*}_x$ the corresponding transmission probability, power and Bernoulli random variable. We write
$$I^{*}(t)=\sum_{x\in X\setminus\lbrace x_o,n(x_o)\rbrace}\textbf{1}_{x}^*P_{x}^*h_t(x,n(x_o))\ell(x,n(x_o)). $$

Since on the event $F$, $I^*(t)$ have the same distribution for all $t\in\mathbb{Z}^+$ then
$$P(A(t)\cup B(t)\vert\mathcal{G}_k)\leq 1-p_{x_o}(1)\varepsilon e^{-\frac{\mu\beta N}{c}}E\lbrace e^{-aI^*(1)}\vert X\rbrace.$$ For $J=p_{x_o}(1)\varepsilon e^{-\frac{\mu\beta N}{c}}E\lbrace e^{-aI^*(1)}\vert X\rbrace$, we get that $$P(T(x_o)>k\vert X)\leq (1-J)^k.$$ Since $0< 1-J< 1 $ the corresponding geometric series converge and by the Cauchy-Schwartz inequality on $J^{-1}$ 
\begin{align*}
E(T_{x_o})&=\sum_{k\geq 0}P(T_{x_o}>k)\\
&=E(\sum_{k\geq 0}P(T_{x_o}>k\vert X))\leq E(J^{-1})\\
&=\leq\frac{e^{\frac{\mu\beta N}{c}}}{\varepsilon}(E\lbrace p_{x_o}(1)^{-2}\rbrace E\lbrace\frac{1}{(E\lbrace e^{-aI^*(1)}\vert X
\rbrace)^2}\rbrace)^{\frac{1}{2}}.
\end{align*}

For finishing the proof we will use a coupling argument, due to Proposition \ref{prop1.1} we can construct an homogeneous Poisson point process $X^{[\lambda_m]}$  with constant intensity $\lambda_m$,  such that $X^{[\lambda_m]}\subset X.$

On the one hand let us denote by $n'(x_o)$ the nearest neighbor of $x_o$ at $X^{[\lambda_m]}$. Since $X^{[\lambda_m]}\subset X$ we have that $\vert n(x_o)-x_o\vert\leq\vert n'(x_o)-x_o\vert$. Hence from the definition of the transmission probability $p_{x_o}(t)$,
\begin{align}
E\lbrace p_{x_o}(1)^{-2}\rbrace &= E\lbrace(\frac{c}{M\ell( x_o, n(x_o))})^{2}\rbrace=(\frac{c}{M})^{2}E(1\vee\vert n(x_o)-x_o\vert^{2\alpha})\nonumber\\
&\leq (\frac{c}{M})^{2}E(1\vee\vert n'(x_o)-x_o\vert^{2\alpha})\nonumber\\
&= \frac{\lambda_m 2\pi }{m}(\frac{c}{M})^{2}\int_0^\infty(1\vee\vert r\vert^{2\alpha})re^{-\frac{\lambda_m \pi }{m}r^2} dr<\infty,\nonumber
\end{align}
since by Proposition \ref{prop1.0} $X^{[\lambda_m]}$ has nearest neighbor density $$f(r)=\frac{\lambda_m 2\pi r}{m}e^{-\frac{\lambda_m \pi }{m}r^2}$$
in the cone $C_d$. 

 On the other hand, by the power control strategy and analogous computations to the already done
\begin{align}
E\lbrace e^{-a\textbf{1}^{*}_x P_x^{*}h_1(x,n(o))\ell(z,n(o))}\vert X\rbrace&= 1-\frac{\beta\gamma p^{*}_x P^{*}_x\ell(x,n(o))}{c+\beta\gamma P^{*}_z\ell(x,n(o))}\nonumber\\
\geq 1-\frac{\beta\gamma M\ell(x,n(o))}{c}&=1-\beta\gamma(1-\varepsilon)\ell(x,n(o)).\nonumber
\end{align}
Let $c_1=\beta\gamma(1-\varepsilon)$, note that since $\beta\gamma<1$ then $0<1-c_1\ell(x,n(x_o))$. Hence we can estimate
\begin{align}
\frac{1}{(E\lbrace e^{-aI^*(1)}\vert X\rbrace)^2} &\leq\prod_{x\in X\setminus\lbrace x_o,n(x_o)\rbrace}\frac{1}{(1-c_1\ell(x,n(x_o)))^2}.\nonumber
\end{align}
Let $Y$ a Poisson point process with intensity function $\lambda\mathds{1}_{(x_o+C_d)\cap B_{\vert n(x_o)\vert}(x_o)}$ independent of $X$ then $(X\setminus\lbrace x_o, n(x_o)\rbrace)\cup Y$ is a Poisson point process of intensity $\lambda$. By Campbell's theorem for Poisson point processes
\begin{align}
E(\frac{1}{(E\lbrace e^{-aI^*(1)}\vert X\rbrace)^2}) &\leq E\prod_{(X\setminus\lbrace x_o, n(x_o)\rbrace)\cup Y}\frac{1}{(1-c_1\ell(x,n(x_o)))^2}\nonumber\\
&=\exp\int_{\mathbb{R}^2}(e^{-2\log(1-c_1\ell(x,n(x_o)))}-1)\lambda(x)dx\nonumber\\
&\leq \exp(\frac{2\lambda_M c_1}{(1-c_1)^2}\int_{\mathbb{R}^2}\ell(x,n(x_o))dx)<\infty.\nonumber
\end{align}
\end{proof}


\section{Positive information velocity }
For finishing this work we offer a result from $\cite{Iye}$ Theorem 3.5 where it is analyzed the asymptotic behavior of the velocity of a message traveling through the network. Before stating it lets first define some key concepts.
\begin{defn}
Let $T_0$ be the time taken by a tagged message starting at $x_o\in X$ to successfully reach its nearest neighbor $x_1=n(x_o)$ in the destination cone $C_1$. More generally let $T_{i-1}$ be the time taken for the message to successfully reach the nearest neighbor $x_i$ of $x_{i-1}$ in the destination cone $x_{i-1}+C_1$. We define the distance $d(t)$ for $t\in\mathbb{Z}^+$ as the random variable that satisfies $$\mathds{1}_{\sum_{i=0}^{k-1}T_i\leq t<\sum_{i=0}^k T_i }d(t)=\vert x_k-{x_o}\vert.$$ We say that the information velocity of SINR network is $$v=\liminf_{t\rightarrow\infty}\frac{d(t)}{t}.$$
\end{defn}

\begin{thm}
Under the conditions of Theorem \ref{teo4.1} if $X$ is a stationary Poisson point process with constant intensity function $\lambda>0$ then the information velocity is almost surely positive. 
\end{thm}
\begin{proof}

For $i\geq 0$ let $r_{i}=\vert x_{i+1}-x_i\vert$, $\theta_{i}=\arcsin(\frac{x_{i+1,2}-x_{i,2}}{r_i})$ where\\ $x_i=(x_{i,1},x_{i,2})$ in $\mathbb{R}^2$	coordinates. The cones $\lbrace (x_i+C_1)\cap B_{r_i}(x_i),i\geq 0\rbrace$ are non-overlapping since $2\phi<\frac{\pi}{2}$. 

Since $X$ is a stationary Poisson point process with intensity $\lambda$, we have that $\lbrace (r_i,\theta_i),i\geq 0\rbrace$ is an i.i.d. sequence of random vectors where $\theta_i$ is uniformly distributed on $(-\phi,\phi)$ and by Proposition \ref{prop1.0} $r_i$ has density $$f(r)=\frac{2\lambda\pi r}{m}e^{-\frac{-\lambda\pi}{m}r^{2}}.$$ Our goal is to construct a stationary sequence of stopping times such that for all $i\geq 0$, $T^{'}_{i}\geq T_i$.



To nullify the effect of moving to the nearest neighbor we progressively fill the voids with independent Poisson points as the packet traverses the network. This however leaves an increasing sequence of special points. The following construction is intended to take care of this issue and deliver an stationary sequence.



\begin{figure}
\captionsetup{width=1\textwidth}
  \centering
    \includegraphics[width=.8\textwidth]{way0}
  \caption{Illustration of the addition of an infinite sequence of points for constructing a stationary sequence of stopping times. Figure taken from \cite{Iye}.} \label{figway0}
\end{figure}

Let $\lbrace (r_{-i},\theta_{-i}),i\geq 1)\rbrace$ an i.i.d. sequence of random vectors with distribution $(r_0,\theta_0)$. Let $\widetilde{X}=\lbrace x_{-i},i\geq 1\rbrace$ starting from $x_{-1}$ such that $r_{-i}=\vert x_{-i}-x_{-i+1}\vert$, $\theta_{-i}=\arcsin(\frac{x_{-i+1,2}-x_{-i,2}}{r_{-i}}).$ See Figure \ref{figway0}.
%$\lbrace \Phi\cap((X_{-i}+C_1)\cap B(X_{-i},R_{-i},i\geq 1)\rbrace$ is a sequence of i.i.d random variables. 

Let $X_i$ be a Poisson point process of intensity $\lambda \textbf{1}_{\lbrace (x_i+C_1)\cap B_{r_i}(x_i)\rbrace}$ independent of everything else, $T_{i}^{'}$ be the delay experienced by the packet in going from $X_i$ to $X_{i+1}$ when the interference is coming from the nodes in $$(X\setminus\lbrace x_i,x_{i+1}\rbrace)\cup\widetilde{X}\cup\bigcup_{j=0}^{i-1}X_j.$$ Then $(T^{'}_{i},i\geq 0)$ is a stationary sequence with $T^{'}_i\geq T_{i}$. We want to proof that $E(T^{'}_0)<\infty$ and then use the Birkoff's ergodic theorem.

Following an analogous reasoning to Theorem \ref{teo4.1} we have that
$$E(T^{'}_0)\leq\frac{e^{\frac{\mu\beta N}{c}}}{\varepsilon}(E\lbrace p_o(1)^{-2}\rbrace E\lbrace\frac{1}{(E\lbrace e^{-a(I^*(1)+\widetilde{I}^*(1))}\vert X\cup\widetilde{ X}\rbrace)^2}\rbrace)^{\frac{1}{2}}
$$ for $\widetilde{I}(t)=\sum_{x\in\widetilde{X}}\textbf{1}_{z}P_z(t)h_{t}(x,n(x_o))\ell(x,n(x_o)).$ We already proved that the first term is finite, lets focus on the second.

$I^{*}(1)$ and $\widetilde{I}^*(1)$ are independent since we conditioned on $X\cup\widetilde{X}$. Then we have that
$$E\lbrace e^{-a(I^*(1)+\widetilde{I}^*(1))}\vert X\cup\widetilde{X}\rbrace=E\lbrace e^{-aI^*(1))}\vert X\rbrace E\lbrace e^{-a\widetilde{I}^*(1)}\vert\widetilde{X}\cup\lbrace n(o)\rbrace\rbrace$$ 
By Cauchy-Schwartz's inequality the result follows if we show that
$$E\lbrace\frac{1}{(E\lbrace e^{-aI^*(1))}\vert X\rbrace)^4}\rbrace E\lbrace\frac{1}{(E\lbrace e^{-a\widetilde{I}^*(1)}\vert\widetilde{X}\cup\lbrace n(o)\rbrace\rbrace)^4}\rbrace <\infty.$$

Analogously to Theorem \ref{teo4.1} by Campbell's theorem the first term can be bounded
$$E\lbrace\frac{1}{(E\lbrace e^{-aI^*(1))}\vert X\rbrace)^4}\rbrace\leq \exp(\frac{\lambda}{(1-c_1)^4}\int_{\mathbb{R}^2}(1-(1-c_1\ell(\vert z\vert))^4)dz)<\infty. $$ 


Lets estimate the second term.  For all $i\in\mathbb{N}$, by Pythagoras' theorem, $$\sum_{j=0}^{i}r_{-j}\cos(\theta_{-j})\leq \vert x_{-i}-n(x_o)\vert.$$ Hence by the last bound since $\ell$ is decreasing
\begin{align*}
E\lbrace\frac{1}{(E\lbrace e^{-a\widetilde{I}^*(1)}\vert\widetilde{X}\cup\lbrace n(x_o)\rbrace\rbrace)^4}\rbrace&\leq E\lbrace\prod_{i=1}^\infty e^{-4\log(1-c_1\ell(x_{-i},n(x_o)))}\rbrace\\
&\leq E\lbrace\prod_{i=1}^\infty e^{-4\log(1-c_1\ell(\sum_{j=0}^{i}r_{-j}\cos(\theta_{-j}))}\rbrace\\
&=E\lbrace e^{\sum_{n=1}^\infty g(S_{n+1})}\rbrace
\end{align*}
where $S_n=\sum_{j=0}^{n-1}r_{-j}\cos(\theta_{-j})$ and $g(x)=-4\log(1-c_1\ell(x))$. 

Let $0<\delta<E\lbrace r_0\cos(\theta_0)\rbrace$, by the Chernoff's bound
$$P(\frac{S_n}{n}<\delta)\leq \min_{t>0}\exp(-n(\delta t+\log E(e^{tS_1})))\leq e^{-n\delta t }.$$ Then by the Borel Cantelli lemma it exists $N$ a random variable such that
\begin{align*}
P(N\geq m)&=P(S_n<n\delta\mbox{ for some }n\geq m)\\
&\leq\sum_{n=m}^{\infty}e^{-\delta t n}=\frac{1}{1-e^{-t\delta}} e^{-t\delta m}.
\end{align*}
Since $g$ is non-increasing we can estimate
\begin{align*}
E\lbrace e^{\sum_{n=1}^\infty g(S_n)}\rbrace&=E\lbrace e^{\sum_{n=1}^N g(S_n)+\sum_{n=N+1}^\infty g(S_n)}\rbrace\\
&\leq E\lbrace e^{\sum_{n=1}^N g(0)+\sum_{n=N+1}^\infty g(n\delta)}\rbrace\leq e^{\sum_{n=1}^\infty g(n\delta)}E\lbrace e^{g(0)N}\rbrace.
\end{align*}
On the one hand by the series comparison test $\sum_{n=1}^\infty g(n\delta)<\infty$.
On the other hand 
$$Ee^{g(0)N}\leq \frac{1}{1-e^{-t\delta}}\sum_{n=0}^\infty e^{n(g(0)-t\delta)} $$
and choosing $t$ big enough the last series converges.

Now that we proved that $E(T'_0)$ is finite then by Birkoff's ergodic theorem there exists a random variable $T^{'}$ such that
$$\lim_{n\rightarrow\infty}\frac{1}{n}\sum_{k=0}^{n-1}T^{'}_k=T^{'}. $$ Also since $T^{'}_i\geq 1$ then $T^{'} \geq 1$. 

Finally from the fact that $\mathds{1}_{\sum_{i=0}^{n-1}T_i\leq t<\sum_{i=0}^n T_i }d(t)\geq \sum_{k=1}^{n-1}R_k\cos(\theta_k)$, we conclude that
$$\liminf_{t\rightarrow\infty}\frac{d(t)}{t}\geq \lim_{n\rightarrow\infty} \frac{\sum_{k=1}^{n-1}R_k\cos(\theta_k)}{\sum_{k=1}^{n-1}T^{'}_k}=\frac{E(R\cos(\theta))}{T^{'}}>0, $$
as we wanted.

\end{proof}
