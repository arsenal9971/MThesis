\chapter{Radial networks}

In this chapter we will analyze the traffic flow generated by a radial navigation on a point process defined on a convex bounded subset of $\mathbb{R}^{d}$. A radial navigation assigns a successor to each node of a point process who lies near a specific target point in space, iterating that process is how the traffic flow is generated.

We will analyze how is the average behavior of the traffic flow on an $\mathbb{R}^{d}$ spherical environment pointing to the target point of the flow. We follow a similar construction to \cite{WIAS} Section 1.2  where the case of a  $\mathbb{R}^{d}$ spherical environment which collapses in to a point is studied. Now we consider the case when it does not collapse.

\section{Radial Navigation}
Let $X^{(s)}$ be a simple point process with intensity $\lambda^{(s)}$ on $sD$ where $D$ is an open, bounded, convex subset of $\mathbb{R}^{d}$; $\lambda^{(s)}(x)=\lambda(\frac{x}{s})$ with $\lambda: D\rightarrow [0,\infty)$ a continuous and bounded mapping with positive maximum and minimum $\lambda_M, \lambda_m$ respectively. We assume that each node $X_i\in X^{(s)}$ generates traffic at rate $\mu^{(s)}(x)=\mu(\frac{x}{s})$ with $\mu: D\rightarrow [0,\infty)$ also a continuous and bounded mapping with maximum and minimum $\mu_M, \mu_m$ respectively.

We want to propose a model where the traffic flow approaches to a target point, for simplicity we will investigate a navigation that approaches to the origin. 
\begin{defn} A measurable function $\mathcal{A}:\mathbb{R}^d\times\textbf{N}\rightarrow\mathbb{R}^d$ is called \textit{radial navigation} (approaching to o)  if for all $\varphi\in\textbf{N}$
\begin{itemize}
\item $\mathcal{A}(o,\varphi\cup \lbrace o\rbrace)=o$ ,
\item $\mathcal{A}(x, \varphi)\in\varphi\cup\lbrace o\rbrace$ for all $x\in\varphi$,
\item $\vert\mathcal{A}(x, \varphi)\vert<\vert x\vert$ for all $x\in\varphi\setminus\lbrace o\rbrace$.
\end{itemize}
If $X$ is a pointed process for $X_{i}\in X\cup\lbrace o\rbrace$, we have that $\mathcal{A}(X_i, X)\in X\cup\lbrace o\rbrace$, so we will simply write $\mathcal{A}(X_i)$.
\end{defn}
\begin{figure}
\captionsetup{width=1\textwidth}
\centering
\includegraphics[width=1\textwidth]{radialnavigation.jpg}
\caption{Radial navigation based on a Poisson point process on the disc. Each node connects to its nearest neighbor which is also closer to the origin, respectively additionally is contained in a cone starting at the node and opening towards the origin. Figure taken from \cite{WIAS}.}
\label{fig.3.1}
\end{figure}
In Figure \ref{fig.3.1} we illustrate two examples of radial navigations, where the successor of a node is the nearest neighbor on a cone with fixed angle pointing to the origin and whose vertex is $X_i$. Each example presents a different angle. 

For the link density condition we need to adapt the definitions of $I^D$ and $\Xi^D$, then for $g(s)$ a positive real valued function, we write $$I^R_s(x)=B^d_{g(s)}(sx)\cap\partial B^d_{s\vert x\vert}(o), $$ which instead of the hyperplane surface in the case of directed networks defines a spherical cap. Moreover, let $$\Xi_s^R(x)=\lbrace X_i\in X^{(s)}:[X_i,\mathcal{A}(X_i)]\cap I_s^R(x)\neq \emptyset\rbrace $$ be the points $X_i$ such that the segment $[X_i,\mathcal{A}(X_i)]$ crosses $I_s^R(x)$. We get the analogous condition

 \begin{con}\label{con3} There exists a function $\lambda_\mathcal{A}(x):D\rightarrow(0,\infty)$ such that for every $x\in D\setminus\lbrace o\rbrace$,
 $$\lambda_\mathcal{A}(x)=\lim_{s\rightarrow\infty}\frac{E\#\Xi_s^R(x)}{s\eta _{d-1}(I_s^R(x))}, $$ where $\eta_{d-1}$ denotes the $(d-1)$-dimensional Hausdorff measure in $\mathbb{R}^d$.
 \end{con}
 
Continuing the analogy, we change the sub-ballisticity condition in such a way that the cylinders point towards the origin. More precisely,  we write $\hat{v}=v/\vert v\vert$ for $v\in\mathbb{R}^d\setminus\lbrace o\rbrace $ and define $$Z_r^R(v)=\lbrace y\in\mathbb{R}^d:\vert y-\langle y,\hat{v}\rangle\hat{v}\vert\leq r\rbrace$$ for the cylinder consisting of all points in $\mathbb{R}^d$ whose projection onto the orthogonal complement of the direction $\hat{v}\in\partial B_1^d(o)$ is of length at most $r\geq 0$. Moreover for $h(s)$ a positive real valued function we write
 $$E^R_s=\lbrace({\Gamma}(X_i)\subset Z_{h(s)}^R(X_i)\mbox{ for all }X_i \in X{(s)}\rbrace, $$ the event that the trajectory is contained in a narrow cylinder towards the origin. An we get the condition 
 \begin{con}\label{con4} $P((E_s^R)^c)\in O(s^{-2d})$.
\end{con} 

\section{Convergence of the traffic flow average in the radial case} 
We are ready to enunciate and prove the main result of this chapter which states that under the link density and sub-ballisticity conditions we will have an almost surely convergence of the traffic flow average on a micro spherical cap orthogonal to the flow. The case when $g(s)/s$ tends monotonically to zero is studied in $\cite{WIAS}$ Theorem 4, we will assume that $g(s)/s$ is constant getting a slightly different result.
\begin{thm} Let $x\in D\setminus\lbrace o\rbrace$. Assume that Conditions \ref{con3},  \ref{con4} are satisfied assume that Conditions \ref{con1}, \ref{con2} are satisfied for $g(s)=s$, $h(s)=s^\xi$,  $0<\xi<1$ and that $E[(\#X^{(s)})^2]\in O(s^{2d})$. 

1.Then, 

$$\lim_{s\rightarrow \infty}\frac{E\sum_{X_i\in\Xi^R_s(x)}\Delta(X_i)}{ E\#\Xi^R_s(x)}=$$ $$\frac{1}{\rho_x\vert x\vert^{d-1}\lambda_\mathcal{A}(x)}\int_{\vert x\vert}^{\infty} r^{d-1}\int_{\partial B^d_1(o)} \mathds{1}_{\vert \hat{y}-\hat{x}\vert \leq 1/\vert x\vert}\mu(ry)\lambda(ry)\eta_{d-1}(dy) dr, $$
where $\rho_x=\int_{\partial B_{1}(o)}\mathds{1}_{\vert\hat{y}-\hat{x}\vert\leq 1/\vert x\vert}\eta_{d-1}(dy)$.


2. If additionally $X^{(s)}$ is either a Poisson point process or $\mu$ is constant on $D$ and $X^{(s)}=X\cap sD$ for some ergodic point process $X$, then 

$$\lim_{s\rightarrow \infty}\frac{\sum_{X_i\in\Xi^R_s(x)}\Delta(X_i)}{E\#\Xi^R_s(x)}=$$ $$\frac{1}{\rho_x\vert x\vert^{d-1}\lambda_\mathcal{A}(x)}\int_{\vert x\vert}^{\infty} r^{d-1}\int_{\partial B^d_1(o)} \mathds{1}_{\vert \hat{y}-\hat{x}\vert \leq 1/\vert x\vert}\mu(ry)\lambda(ry)\eta_{d-1}(dy) dr, $$
in probability.
\
\end{thm} 
\begin{proof}
By a similar procedure that in the directed case first we have to give estimations of the accumulated traffic flow under $E^R_s$ and $(E^R_s)^c$. Let us define the analogous of the cylinders $R^{l,+}_s(x)$ and $R^{l,-}(x)$. First, $$C^+_s(x)=\lbrace sy\in sD\setminus B^d_{\vert sx\vert}(o):\vert\hat{y}-\hat{x}\vert\leq(g(s)+2h(s))/s\vert x\vert\rbrace $$ is the set of points which lie in an extended cone around $sx$ facing towards the boundary of $sD$. Second for $s$ such that $g(s)\geq 2h(s)$
 $$C^-_s(x)=\lbrace sy\in sD\setminus B^d_{\vert sx\vert}(o):\vert\hat{y}-\hat{x}\vert\leq(g(s)-2h(s))/s\vert x\vert\rbrace $$ the set of points which lie in a diminished cone around $sx$ facing towards the boundary of $sD$, see also Figure \ref{fig.3.3}.  
 %Let also $\mu_{M}=\sup_{x\in D}\mu(x)$ and $\lambda_{M}=\sup_{x\in D}\mu(x)$ which are finite by boundedness assumptions.
 \begin{figure}
 \captionsetup{width=1\textwidth}
\centering
\includegraphics[width=.7\textwidth]{con.jpg}
\caption{Construction of the cones $C^-_s(x)$ (dark gray) and $C^+_s(x)$ (union of light and dark gray). Figure took from \cite{WIAS}.}
\label{fig.3.3}
\end{figure}

\begin{lem}\label{lem3.1}
 Let $x\in D$ and $X^{(s)}$ on the event $E_{s}^R$ then
 $$\sum_{X_j\in X^{(s)}\cap C^-_s(x)}\mu^{(s)}(X_j)\leq\sum_{X_j\in  \Xi^{R}_{s}(x)}\Delta^{(s)}(X_j)\leq\sum_{X_j\in X^{(s)}\cap C^{+}_{s}(x)}\mu^{(s)}(X_j).$$
 \end{lem}
\begin{proof}
For the upper bound, lets observe that if we pick $X_i \in \Xi_s^R(x)$ on the event $E^{R}_s$ we have that $\Gamma(X_i)\subset Z^R_{h(s)}(X_i)\subset C^{+}_s(x)$ and in consequence
 $$\sum_{X_j\in  \Xi^{R}_{s}(x)}\Delta^{(s)}(X_j)\leq\sum_{X_j\in X^{(s)}\cap C^{+}_{s}(x)}\mu^{(s)}(X_j).$$
 For the lower bound, lets pick $X_j\in X^{(s)}\cap C^-_s(x)$. On the one hand since $\mathcal{A}$ is a radial navigation there exists $X_{j}\in X^{(s)}$ s.t. $[X_{j},\mathcal{A}(X_j)]\cap B^d_{\vert sx\vert}(o)\neq \emptyset$ and $X_i\in\Gamma (X_j)$. On the other hand $\Gamma (X_j)\subset Z^R_{h(s)}(X_j)$. Which implies that $X_i\in \Xi_s^R$  and we conclude 
$$\sum_{X_j\in X^{(s)}\cap C^-_s(x)}\mu^{(s)}(X_j)\leq\sum_{X_j\in  \Xi^{R}_{s}(x)}\Delta^{(s)}(X_j).$$
 \end{proof} 
 
 The following lemma is analogous to Lemma \ref{lem2.2}
 \begin{lem}\label{lem3.2} $E(\mathds{1}_{(E^{R}_s)^c}\sum_{X_i\in\Xi_s^D(x)}\Delta(X_i))\in O(1)$.
 \end{lem}
We proceed to proof the theorem. For that let $$i_{s, x}=s\eta_{d-1}(I^R_s(x)), N_s=(i_{s, x})^{-1}\sum_{X_j\in  \Xi^{R}_{s}(x)}\Delta^{(s)}(X_j),$$ $$ S=\frac{1}{\rho_x\vert x\vert^{d-1}}\int_{\vert x\vert}^{\infty} r^{d-1}\int_{\partial B^d_1(o)} \mathds{1}_{\vert \hat{y}-\hat{x}\vert \leq 1}\mu(ry)\lambda(ry)\eta_{d-1}(dy) dr.$$
 
\textit{Proof of Theorem.}\textbf{1.} By Condition \ref{con3} it is enough to proof that $\lim_{s\rightarrow\infty}EN_s=S$. We have that
 $$EN_s=E\mathds{1}_{E^{R}_s}N_s+E\mathds{1}_{(E^{R}_s)^c}N_s .$$

First lets estimate  
\begin{align}
i_{s, x}=s\eta_{d-1}((I^D_s(x))&=s\int_{\partial B_{\vert sx\vert}(o)}\mathds{1}_{\vert\hat{y}-\hat{x}\vert\leq g(s)/\vert sx\vert}\eta_{d-1}(dy)\nonumber\\
&=s^d\rho_x\vert x\vert^{d-1},\nonumber
\end{align} 
and then $i_{s, x}^{-1}\in o(1) $. So by Lemma \ref{lem3.2} $E\mathds{1}_{(E^{R}_s)^c}N_s\in o(1)$.

Lets find an upper found for $E\mathds{1}_{E^{R}_s}N_s$. By Lemma \ref{lem3.1} and Campbell's theorem 

 \begin{align}
 E\mathds{1}_{E^{R}_s}N_s&\leq i_{s, x}^{-1}E\sum_{X_j\in X^{(s)}\cap C^+_s(x)}\mu^{(s)}(X_j)\nonumber\\
 &=i_{s, x}^{-1}\int_{C^+_s(x)}\mu^{(s)}(y)\lambda^{(s)}(y)dy\nonumber.
 \end{align}
By the coordinate transformation $y=sz$, the co-area formula applied to the function $f(x)=\Vert x\Vert$ and the transformation $y=rz$ we rewrite
\begin{align}
\int_{C^+_s(x)}&\mu^{(s)}(y)\lambda^{(s)}(y)dy=\int \mathds{1}_{\vert y\vert >\vert sx\vert}\mathds{1}_{\vert sx \vert\vert \hat{y}-\hat{x}\vert \leq g(s)+2h(s))}\mu^{(s)}(y)\lambda^{(s)}(y)dy\nonumber\\
&=s^d\int \mathds{1}_{\vert y\vert >\vert x\vert}\mathds{1}_{\vert \hat{y}-\hat{x}\vert \leq (g(s)+2h(s))/\vert sx\vert}\mu(y)\lambda(y)dy\nonumber\\
&=s^d\int_{\vert x\vert}^{\infty}\int_{\partial B^d_r(o)} \mathds{1}_{\vert \hat{y}-\hat{x}\vert \leq (g(s)+2h(s))/\vert sx\vert}\mu(y)\lambda(y)\eta_{d-1}(dy)dr\nonumber\\
&=s^d\int_{\vert x\vert}^{\infty} r^{d-1}\int_{\partial B^d_1(o)}\mathds{1}_{\vert \hat{y}-\hat{x}\vert \leq (g(s)+2h(s))/\vert sx\vert}\mu(ry)\lambda(ry)\eta_{d-1}(dy)dr\nonumber
\end{align}
And multiplying by $i_{s, x}^{-1}$ we get the final bound
$$\frac{1}{\rho_x\vert x\vert^{d-1}}\int_{\vert x\vert}^{\infty} r^{d-1}\int_{\partial B^d_1(o)} \mathds{1}_{\vert \hat{y}-\hat{x}\vert \leq (g(s)+2h(s))/\vert sx\vert}\mu(ry)\lambda(ry)\eta_{d-1}(dy) dr.$$ 
Which by the Dominated Convergence theorem converges to $S$.


For the lower bound, using the same arguments as above we can estimate
\small $$E\mathds{1}_{E^{R}_s}N_s\geq s^d i_{s, x}^{-1}\int_{\vert x\vert}^{\infty} r^{d-1}\int_{\partial B^d_1(o)}\mathds{1}_{\vert \hat{y}-\hat{x}\vert \leq (g(s)-2h(s))/\vert sx\vert}\mu(ry)\lambda(ry)\eta_{d-1}(dy)dr. $$
\normalsize And using an analogous reasoning that in the upper bound the result follows.

\textbf{2.} Again by Condition \ref{con3} it suffices to show that $$\lim_{s\rightarrow\infty}N_s=S $$ in probability. Following the same arguments as in Theorem \ref{thm2.1} 2. replacing by the following definition $$N^{\pm}_s=i_{s,x}^{-1}\sum_{X_j\in X^{(s)}\cap C^\pm_s(x)}\mu^{(s)}(X_j) $$ it suffices to show that $E(\vert N^+_s-EN^+_s\vert)$ and $E(\vert N^-_s-EN^-_s\vert)$ tends to zero as $s$ tends to infinity for a Poisson and an ergodic point process.

\textbf{Case 1)} Let $X^{(s)}$ be a Poisson point process, then by Jensen's inequality, Campbell's theorem for the variance
\begin{align}
E(\vert N_s^+&-EN_s^+\vert)\leq(VN^+_s)^{1/2}=i_{s,x}^{-1} V(\sum_{X_j\in X^{(s)}\cap C^+_s(x)}\mu^{(s)}(X_j))^{1/2}\nonumber\\
&=i^{-1/2}_{s,x}(i^{-1}_{s,x}\int_{C^+_s(x)}(\mu^{(s)}(y))^2\lambda^{(s)}(y)dy)^{1/2}\nonumber.
\end{align}
And by an analogous reasoning to 1. of we get that when $s$ tends to infinity the second term converges to a constant and hence the complete expression goes to zero. The reasoning for $E(\vert N_s^--EN_s^-\vert)$ is similar.

\textbf{Case 2)} Lets assume that $X$ is ergodic on $\mathbb{R}^d$ and $X^{(s)}=X\cap sD$. Let $r^+_s(x)=\nu_d(C^+_s(x))$. Then by ergodicity $X$ has constant density $\lambda$. Since by hypothesis also $\lambda$ is constant by Campbell's theorem we compute $$E(\vert N_s^+-EN_s^+\vert)=\mu\frac{r^+_s(x)}{i_{s, x}}E(\vert\frac{\#(X\cap C^+_s(x))}{r^+_s(x)}-\lambda\vert), $$ where analogously to computations done in 1. we have that $\frac{r^+_s(x)}{i_{s, x}}$ converges to a constant when $s$ tends to infinity
\begin{figure}
\captionsetup{width=1\textwidth}
\centering
\includegraphics[width=1\textwidth]{averwind.jpg}
\caption{Construction of the cone $\hat{C}^+_{s,x}$ (gray area), where $\langle z, \hat{x}\rangle=\zeta_{s,x}$. Figure took from \cite{WIAS}.}
\label{fig3.4}
\end{figure}
We have to modify $C_s^+(x)$ to become a sequence of convex averaging windows. Let $$\zeta_{s,x}=\inf\lbrace\langle z, \hat{x}\rangle: z\in\partial D\cap(C^+_s(x)/s)\rbrace$$ the smallest component in the direction $\hat{x}$ of points in the boundary $sD$ intersected with the cone $C_s^+(x)$. 
Let $$\dot{C}^+_s(x)=\lbrace sy\in sD: sy\in C^+_s(x)\mbox{ and }\vert x\vert\leq\langle y,\hat{x}\rangle\leq\zeta_{s,x}\rbrace,\hat{C}^+_s(x)=\dot{C}^+_s(x)-sx.$$ $\hat{C}^+_s(x)$ is a sequence of convex averaging windows since $g(s)$ and $h(s)$ are monotonically increasing, see also Figure \ref{fig3.4} .

Let $r^+_s(x)=\eta_d(\hat{C}^+_s(x))$,  by stationarity we can estimate\small $$E(\vert\frac{\#(X\cap C^+_s(x))}{r^+_s(x)}-\lambda\vert)\leq E(\vert\frac{\#(X\cap  C^+_s(x))}{r^+_s(x)}-\frac{\#(X\cap(\dot{C}^+_s(x))}{\hat{r}^+_s(x)}\vert)+ E(\vert\frac{\#(X\cap\hat{C}^+_s(x)}{\hat{r}^+_s(x)}-\lambda\vert). $$ \normalsize
Due to the ergodic theorem second summand tends to zero when $s$ tends to infinity. Lets proceed to estimate the first one. 

Since $\hat{r}^+_s(x)\leq r^+_s(x)$ and $\#(X\cap\dot{C}^+_s(x))\leq \#(X\cap C^+_s(x))$ we have that
 \begin{align}
\#(X\cap\dot{C}^+_s(x))(\frac{1}{r^+_s(x)}-\frac{1}{\hat{r}^+_s(x)})&\leq \frac{\#(X\cap C^+_s(x))}{r^+_s(x)}-\frac{\#(X\cap\dot{C}^+_s(x))}{\hat{r}^+_s(x)}\nonumber\\
&\leq \frac{\#(X\cap C^+_s(x))-\#(X\cap\dot{C}^+_s(x))}{\hat{r}^+_s(x)},\nonumber 
\end{align}
and in consequence by Campbell's theorem
\begin{align}
&E(\vert\frac{\#(X\cap  C^+_s(x))}{r^+_s(x)}-\frac{\#(X\cap(\dot{C}^+_s(x))}{\hat{r}^+_s(x)}\vert)\nonumber\\
&\leq E(\vert\frac{\#(X\cap C^+_s(x))-\#(X\cap\dot{C}^+_s(x))}{\hat{r}^+_s(x)}\vert)+E(\vert \#(X\cap\dot{C}^+_s(x))(\frac{1}{r^+_s(x)}-\frac{1}{\hat{r}^+_s(x)})\vert)\nonumber\\
&\leq 2\lambda \frac{r^+_s(x)-\hat{r}_s^+(x)}{\hat{r}^+_s(x)}\nonumber .
\end{align}
Which tends to zero when $s$ tends to infinity. Similar arguments apply for the case where $N^+$ is replaced by $N^-$.
\end{proof}